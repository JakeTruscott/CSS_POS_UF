---
title: "Embeddings"
subtitle: "POS6933: Computational Social Science"
author: "Truscott (Spring 2026)"
output:
  html_document:
    self_contained: false
    layout: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(ggplot2)
library(dplyr)
library(cowplot)
library(stargazer)
library(doParallel)
library(parallel)
library(snow)
library(tm)
library(quanteda)
library(gutenbergr)
library(stringr)

```

------------------------------------------------------------------------

## Embeddings: Or, How I Learned to Stop Worrying and Embrace Distributed Representations of Words

To this point, our analyses of text have largely relied on one-hot encoding. From rudimentary co-occurrence measures using dictionaries to more advanced approaches like Naive Bayes, we have remained firmly within the Bag-of-Words (BoW) framework. As we noted when first introducing BoW, even with careful preprocessing and complexity reduction, this approach has an important limitation: it provides no way to represent similarity between words.

Consider the matrix below, which represents a one-hot encoding of a small vocabulary:

|           | Dog | Cat | Horse | Pig |
|:---------:|:---:|:---:|:-----:|:---:|
|  **Dog**  |  1  |  0  |   0   |  0  |
|  **Cat**  |  0  |  1  |   0   |  0  |
| **Horse** |  0  |  0  |   1   |  0  |
|  **Pig**  |  0  |  0  |   0   |  1  |

Notwithstanding the diagonal, all entries are zero. The vector for *cat* would then be $\text{cat} = (0, 1, 0, 0)$, which conveys only the fact that the word is cat and nothing else. For some tasks -- e.g., identifying whether a document from a veterinary hospital discusses a particular animal, this may be sufficient.

But what if we have a vocabulary like this?

|                | Dog | Pooch | K9  | Fire Truck |
|:--------------:|:---:|:-----:|:---:|:----------:|
|    **Dog**     |  1  |   0   |  0  |     0      |
|   **Pooch**    |  0  |   1   |  0  |     0      |
|     **K9**     |  0  |   0   |  1  |     0      |
| **Fire Truck** |  0  |   0   |  0  |     1      |

Suddenly we're at risk of some information loss â€“ because *Dog*, *Pooch*, and *K9* are reasonably describing the same thing. Put a different way, if we were to approach this as a clustering task, how would we organize this vocabulary? Certainly we wouldn't want to say each word represents it's own cluster center. Rather, the assortment would probably be $K_{1} = (\text{Dog, Pooch, K9})$ and $K_{2} = (\text{Fire Truck})$. Yet this structure is invisible in a one-hot representation, where all distinct words are equally dissimilar.

So, I am going to posit we consider a few things:

1.  **Words have similar meaning**. Recall that we discussed a few classes ago just how broad and diverse the English language is. Move from a dictionary to a thesaurus and you'll quickly realize that virtually every word has a worthy synonym to replace it given syntax or other some reasoning. Word choice is certainly an important indicator of author's intent (it was good enough to attribute authorship of disputed *Federalist Papers*!), but treating each word as entirely distinct is often too restrictive. Moreover, many words are polysemous, taking on different meanings depending on context. A good example (I think) from my own research is *Judge* -- which, given context as a verb (*to judge*) versus a noun (*a judge*) can prescribe very different meanings.

2.  **Similarity is not fully captured in word co-occurrence**: Rather than asking whether two words are exactly the same, a more useful question is whether they tend to appear in similar contexts. If we were to compare words across a corpus, we would expect *Pooch* to appear in usage settings much closer to *Dog* than to *Fire Truck*. One-hot encodings cannot represent this graded notion of similarity.

## Distributed Representations of Words

**Distributed Representations of Words** are dense vectors that replace the one-hot encoding representation of words that build in some measure of similarity. In short, rather than a sparse vector of mostly-zeros to represent a word, we'll use data to construct a vector of length *K* (where *K* is less than *J* -- the total number of unique words int he vocabulary). Reducing the dimensionality like this is what allows **word embeddings** to capture similarity structure (e.g., *semantic* or *syntactic* relationships) efficiently. The key advantage is that the representation is no longer (essentially) dichotomous, where a word is coded as 0/1 in a high-dimensional sparse vector. Instead, we construct a low-dimensional representation of each word, such that its position along each dimension can be used to infer similarity to other words. As *GSR* note, these distributed representations are typically learned through the *distributional hypothesis*, which holds that words appearing in similar contexts tend to have similar meanings. In this sense, word embeddings operationalize contextual co-occurrence patterns in a way that moves beyond the independence assumptions of the traditional Bag of Words approach.
