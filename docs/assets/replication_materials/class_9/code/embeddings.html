<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Embeddings</title>

<script src="embeddings_files/header-attrs-2.28/header-attrs.js"></script>
<script src="embeddings_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="embeddings_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="embeddings_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="embeddings_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="embeddings_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="embeddings_files/navigation-1.1/tabsets.js"></script>
<link href="embeddings_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="embeddings_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Embeddings</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div
id="embeddings-or-how-i-learned-to-stop-worrying-and-embrace-distributed-representations-of-words"
class="section level2">
<h2>Embeddings: Or, How I Learned to Stop Worrying and Embrace
Distributed Representations of Words</h2>
<p>To this point, our analyses of text have largely relied on one-hot
encoding. From rudimentary co-occurrence measures using dictionaries to
more advanced approaches like Naive Bayes, we have remained firmly
within the Bag-of-Words (BoW) framework. As we noted when first
introducing BoW, even with careful preprocessing and complexity
reduction, this approach has an important limitation: it provides no way
to represent similarity between words.</p>
<p>Consider the matrix below, which represents a one-hot encoding of a
small vocabulary:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Dog</th>
<th align="center">Cat</th>
<th align="center">Horse</th>
<th align="center">Pig</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Dog</strong></td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Cat</strong></td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center"><strong>Horse</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Pig</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>Notwithstanding the diagonal, all entries are zero. The vector for
<em>cat</em> would then be <span class="math inline">\(\text{cat} = (0,
1, 0, 0)\)</span>, which conveys only the fact that the word is cat and
nothing else. For some tasks – e.g., identifying whether a document from
a veterinary hospital discusses a particular animal, this may be
sufficient.</p>
<p>But what if we have a vocabulary like this?</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Dog</th>
<th align="center">Pooch</th>
<th align="center">K9</th>
<th align="center">Fire Truck</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Dog</strong></td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Pooch</strong></td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center"><strong>K9</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Fire Truck</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>Suddenly we’re at risk of some information loss – because
<em>Dog</em>, <em>Pooch</em>, and <em>K9</em> are reasonably describing
the same thing. Put a different way, if we were to approach this as a
clustering task, how would we organize this vocabulary? Certainly we
wouldn’t want to say each word represents it’s own cluster center.
Rather, the assortment would probably be <span
class="math inline">\(K_{1} = (\text{Dog, Pooch, K9})\)</span> and <span
class="math inline">\(K_{2} = (\text{Fire Truck})\)</span>. Yet this
structure is invisible in a one-hot representation, where all distinct
words are equally dissimilar.</p>
<p>So, I am going to posit we consider a few things:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Words have similar meaning</strong>. Recall that we
discussed a few classes ago just how broad and diverse the English
language is. Move from a dictionary to a thesaurus and you’ll quickly
realize that virtually every word has a worthy synonym to replace it
given syntax or other some reasoning. Word choice is certainly an
important indicator of author’s intent (it was good enough to attribute
authorship of disputed <em>Federalist Papers</em>!), but treating each
word as entirely distinct is often too restrictive. Moreover, many words
are polysemous, taking on different meanings depending on context. A
good example (I think) from my own research is <em>Judge</em> – which,
given context as a verb (<em>to judge</em>) versus a noun (<em>a
judge</em>) can prescribe very different meanings.</p></li>
<li><p><strong>Similarity is not fully captured in word
co-occurrence</strong>: Rather than asking whether two words are exactly
the same, a more useful question is whether they tend to appear in
similar contexts. If we were to compare words across a corpus, we would
expect <em>Pooch</em> to appear in usage settings much closer to
<em>Dog</em> than to <em>Fire Truck</em>. One-hot encodings cannot
represent this graded notion of similarity.</p></li>
</ol>
</div>
<div id="distributed-representations-of-words" class="section level2">
<h2>Distributed Representations of Words</h2>
<p><strong>Distributed Representations of Words</strong> are dense
vectors that replace the one-hot encoding representation of words that
build in some measure of similarity. In short, rather than a sparse
vector of mostly-zeros to represent a word, we’ll use data to construct
a vector of length <em>K</em> (where <em>K</em> is less than <em>J</em>
– the total number of unique words int he vocabulary). Reducing the
dimensionality like this is what allows <strong>word embeddings</strong>
to capture similarity structure (e.g., <em>semantic</em> or
<em>syntactic</em> relationships) efficiently. The key advantage is that
the representation is no longer (essentially) dichotomous, where a word
is coded as 0/1 in a high-dimensional sparse vector. Instead, we
construct a low-dimensional representation of each word, such that its
position along each dimension can be used to infer similarity to other
words. As <em>GSR</em> note, these distributed representations are
typically learned through the <em>distributional hypothesis</em>, which
holds that words appearing in similar contexts tend to have similar
meanings. In this sense, word embeddings operationalize contextual
co-occurrence patterns in a way that moves beyond the independence
assumptions of the traditional Bag of Words approach.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
