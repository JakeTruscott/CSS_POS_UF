<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Multinomial Language Model</title>

<script src="multinomial_language_model_files/header-attrs-2.28/header-attrs.js"></script>
<script src="multinomial_language_model_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="multinomial_language_model_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="multinomial_language_model_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="multinomial_language_model_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="multinomial_language_model_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="multinomial_language_model_files/navigation-1.1/tabsets.js"></script>
<link href="multinomial_language_model_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="multinomial_language_model_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Multinomial Language Model</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="multinomial-language-model" class="section level2">
<h2>Multinomial Language Model</h2>
<p>The multinomial language model is a probabilistic framework used to
model the distribution of words in a document or collection of
documents. It assumes that each word in a document is drawn
independently from a fixed vocabulary according to a categorical
distribution – i.e., in accordance with the <i>Bag of Words</i>
approach, where each word has a certain probability of occurring. The
model is <i>multinomial</i> in that it considers the counts of each word
in a document, rather than just the presence or absence of words.
Formally, for a document represented as a sequence of word counts, the
likelihood of observing the document is given by the multinomial
probability mass function (PMF), which combines the factorial of the
total word count with the product of the probabilities of each word
raised to the power of its observed count. This framework forms the
basis for many text modeling techniques, including Naive Bayes
classifiers and topic models (which we will explore more this week and
next…), and provides a straightforward way to estimate the probability
of unseen documents given observed word frequencies.</p>
<hr />
<p><b>GRS</b> (Ch. 6) uses a simplified three-word vocabulary
<code>(cat, dog, fish)</code> and each document only contains a single
token (i.e., instance of a type). We are going to retains a similar
structure, but add another word to our vocabulary: <br></p>
<div style="text-align:center;">
<p><code>hamburger = (1, 0, 0, 0)</code><br></p>
<p><code>salad = (0, 1, 0, 0)</code><br></p>
<p><code>taco = (0, 0, 1, 0)</code><br></p>
<p><code>nuggets = (0, 0, 0, 1)</code><br></p>
</div>
<p>Recall that this approach accords with the Bag of Words, where words
are drawn individually and independently from a categorical
distribution, where <span class="math inline">\(W_i = \mu\)</span> –
where <span class="math inline">\(\mu\)</span> is <i>a vector containing
the probability of each individual type</i>. For this example, lets say
<span class="math inline">\(\mu =\)</span> (0.3, 0.25, 0.15, 0.3).
Meaning that the probability of each token type being drawn for any
trial is:</p>
<ul>
<li><p><span class="math inline">\(p\)</span>(<code>hamburger</code>) =
0.3</p></li>
<li><p><span class="math inline">\(p\)</span>(<code>salad</code>) =
0.25</p></li>
<li><p><span class="math inline">\(p\)</span>(<code>taco</code>) =
0.15</p></li>
<li><p><span class="math inline">\(p\)</span>(<code>nuggets</code>) =
0.3</p></li>
</ul>
<p>In other words, each word in the document is generated by
independently sampling from these four categories according to their
respective probabilities. Let’s assume we were interested in the
probability of drawing the document (<code>hamburger</code>,
<code>hamburger</code>, <code>taco</code>, <code>nuggets</code>). The
resulting count vector would be (2, 0, 1, 1) – representing 2 instances
of <code>hamburger</code>, 0 instances of <code>salad</code>, and 1
instance of both <code>taco</code> and <code>nuggets</code>.</p>
<p>Recall that the probability mass function for a categorical
distribution is:</p>
<p><span class="math display">\[
p(\mathbf{W}_i \mid \boldsymbol{\mu})
= \prod_{j=1}^J \mu_j^{w_{ij}}
\]</span></p>
<p>which we can generalize for documents that are longer than one word
using the multinomial distribution, where <span
class="math inline">\(\mathbf{M}\)</span> is an integer that controls
the number of tokens (i.e., length of the document):</p>
<p><span class="math display">\[
p(\mathbf{W}_i \mid \boldsymbol{\mu}) =
\frac{M!}{\prod_{j=1}^J W_{ij}!} \prod_{j=1}^J
\mu_{j}^{\mathbf{W}_{ij}}
\]</span></p>
<p>Supplementing our values for the hypothetical document
(<code>hamburger</code>, <code>hamburger</code>, <code>taco</code>,
<code>nuggets</code>), we get:</p>
<p><span class="math display">\[
p(\texttt{H,H,T,N} \mid \mu) =
\frac{4!}{(2_{H}!)(0_{S}!)(1_{T}!)(1_{N}!)}
(0.3_H)^2 (0.25_S)^0 (0.15_T)^1 (0.3_N)^1
\]</span></p>
<p><span class="math display">\[
= \frac{4!}{2!\cdot0!\cdot1!\cdot1!}\quad 0.09 \cdot 1 \cdot 0.15 \cdot
0.3
\]</span></p>
<p><span class="math display">\[
= 12 \cdot 0.00406 \\
\]</span> <span class="math display">\[
p(H,H,T,N \mid \mu) \approx 0.0486
\]</span></p>
<hr />
<p>As <b>GRS</b> (Ch.6) also note, the advantage of specifying a
probability model is it accompanies a set of known results that are a
consequence of the modeling assumptions – e.g., expectation, variance,
and covariance.</p>
<p><span class="math display">\[
\text{Expected Number of Times Word } j \text{ appears in Document } i
\]</span> <span class="math display">\[
E[\mathbf{W_{ij}}] = M_i\mu_j
\]</span> <br> <span class="math display">\[
\text{Variance of the count of word } j \text{ in Document } i
\]</span></p>
<p><span class="math display">\[
\text{Var}(\mathbf{W}_{ij{}}) = M_i\mu_j(1-\mu_j)
\]</span> <br> <span class="math display">\[
\text{Covariance of the count of word } j \text{ given word } k  \text{
in Document } i; j \neq k
\]</span> <span class="math display">\[
\text{Cov}(\mathbf{W}_{ij}, \mathbf{W}_{ik}) = -M_i\mu_j\mu_k
\]</span> <br></p>
<p>Putting it all together…</p>
<pre class="r"><code>mu &lt;- c(hamburger = 0.3,
        salad     = 0.25,
        taco      = 0.15,
        nuggets   = 0.3)  # Mu (Probs)

M &lt;- 4  # Document length (number of tokens)


expectation_wij &lt;- M * mu # i.e., If I repeatedly generated documents of length M from mu, this is the average number of times each word would appear.

expectation_wij</code></pre>
<pre><code>## hamburger     salad      taco   nuggets 
##       1.2       1.0       0.6       1.2</code></pre>
<pre class="r"><code>variance_wij &lt;- M * mu * (1 - mu) # i.e., How much the count of each word bounces around from document to document -- Does it show up a lot (1), never (0), or 50/50 (0.5)? 

variance_wij</code></pre>
<pre><code>## hamburger     salad      taco   nuggets 
##      0.84      0.75      0.51      0.84</code></pre>
<pre class="r"><code>covariance_wij &lt;- -M * (mu %o% mu)  # i.e., How much is the variance of one word related to another? We use off-diagonal elements -- how counts of *different* words move together (all sum to M!)


diag(covariance_wij) &lt;- variance_wij # Note: outer-product for off-diagonal gives us wrong values for diagonal (M * mu_j * mu_k) -- so we replace with true variance for W_ij 

covariance_wij</code></pre>
<pre><code>##           hamburger salad  taco nuggets
## hamburger      0.84 -0.30 -0.18   -0.36
## salad         -0.30  0.75 -0.15   -0.30
## taco          -0.18 -0.15  0.51   -0.18
## nuggets       -0.36 -0.30 -0.18    0.84</code></pre>
<hr />
<div id="example-unaccredited-federalist-papers" class="section level3">
<h3>Example: Unaccredited Federalist Papers</h3>
<p><b>GSR</b> (Ch.6) provide an illustrative example from <a
href="https://www.jstor.org/stable/pdf/2283270.pdf?casa_token=OoOmdwxe1bUAAAAA:FdpJImtxN8WZtE_qSHo_K4eWTT2QfenadrcR9DuabjyvfLVthPoesEBgQWONCeuJkJaSB3xy-nMqTI8KswcB6XrzUrbFaH-kjJrcvkjw60yF9weCsNU">Mosteller
and Wallace (1963)</a> concerning the disputed authorship of 12
Federalist Papers – a collection of 85 essays authored by Alexander
Hamilton, James Madison, and John Jay between October 1787 and August
1788 advocating for the ratification of the U.S. Constitution. Since
each Paper was authored using the same collective pseudonym –
<i>Publius</i> – authorship was disputed. Or, at least, debate was
surely had concerning which of the three – Hamilton, Madison, or Jay –
was a particular Paper’s author. Some were later discovred to have been
authored jointly. However, by the mid-20th century, it was believed that
Jay authored (5), Hamilton authored at least (43), and Madison authored
(14).</p>
<p>Mosteller and Wallace (1963) were able to use basic notions from the
<i>Bag of Words</i> and multinomial language models to infer authorship
for those disputed papers given variance in the writing styles of each
known author. Given variance in the writing styles of Hamilton, Madison,
and Jay – e.g., use of specific terminology, phrasing, etc. – we can
infer authorship of those without attribution. Most importantly, we
assume these authors’ behaviors represent distinct data generating
processes – and, by virtue, unique multinomial distribution and
generative process <span class="math inline">\(\mu_{H,M,J}\)</span>
(<i>See <b>GSR</b> Table 6.1</i>). However, we’re again going to add a
bit to <b>GSR</b>’s example and use an five-word vocabulary</p>
<p>Let’s start by recovering the <i>Federalist Papers</i>, reducing
complexity, and isolating to instances where any of the authors use the
terms: <i>by, man, upon, heretofore</i> or <i>whilst</i>. Doing so, we
recover the unique counts below:</p>
<pre class="r"><code>tidy_federalist %&gt;%
  count(author, word) %&gt;%          
  tidyr:: pivot_wider(names_from = author,values_from = n, values_fill = 0) %&gt;%
  { 
    wide &lt;- .                      
    bind_rows(
      wide,
      wide %&gt;%
        select(-word) %&gt;%
        summarise(across(everything(), sum)) %&gt;%
        mutate(word = &quot;TOTAL&quot;) %&gt;%
        select(word, everything()))
  } # Print Counts of Interesting Words</code></pre>
<pre><code>## # A tibble: 6 × 4
##   word       Hamilton   Jay Madison
##   &lt;chr&gt;         &lt;int&gt; &lt;int&gt;   &lt;int&gt;
## 1 by              861    82     477
## 2 heretofore       13     1       1
## 3 man             102     0      17
## 4 upon            374     1       7
## 5 whilst            1     0      12
## 6 TOTAL          1351    84     514</code></pre>
<p>Producing the following sum of multinomials, where the values x in
Multinomial(x, <span class="math inline">\(\mu\)</span>) represent the
count of the observed vocabulary for each prospective author : <span
class="math display">\[
\mathbf{W_{Hamilton}} ~ \text{Multinomial}(1351, \mu_H)
\]</span> <span class="math display">\[
\mathbf{W_{Madison}} ~ \text{Multinomial}(514, \mu_M)
\]</span> <span class="math display">\[
\mathbf{W_{Jay}} ~ \text{Multinomial}(84, \mu_J)
\]</span></p>
<p>We then turn towards recovering the unique <span
class="math inline">\(\mu_H,M,J\)</span> for each author – This example
recovers Hamilton’s:</p>
<p><span class="math display">\[
\mu_{Hamilton} =
(\frac{861}{861+13+102+374+1},\frac{13}{1351},\frac{102}{1351},
\frac{374}{1351}, \frac{1}{1351})
\]</span></p>
<p><span class="math display">\[
\mu_{Hamilton} = (0.63, 0.009, 0.07, 0.27, 0.0007)
\]</span></p>
<p>The others are:</p>
<p><span class="math display">\[
\mu_{Madison} = (0.92, 0.001, 0.033, 0.013, 0.023)
\]</span> <span class="math display">\[
\mu_{Jay} = (0.97, 0.01, 0, 0.01, 0)
\]</span></p>
<p>We’re going to apply these author-specific <span
class="math inline">\(\mu\)</span> to two examples – <i>Federalist
51</i>, where authorship was disputed between Madison and Hamilton prior
to Mosteller and Wallace (1963), and another where we know authorship
(as a robustness check).</p>
<p>For <i>Federalist 51</i>, we know the following counts:</p>
<ul>
<li><p>by (23)</p></li>
<li><p>man (1)</p></li>
<li><p>upon (0)</p></li>
<li><p>heretofore (0)</p></li>
<li><p>whilst (2)</p></li>
</ul>
<p>Applying our known <span class="math inline">\(\mu\)</span> for each
author:</p>
<p><span class="math display">\[
p(\mathbf{W_{Fed51}}\mid\mu_{Hamilton}) =
\frac{26!}{(23!)(1!)(0!)(0!)(2!)}(0.63)^{23}(0.009)^{1}(0.07)^{0}(0.27)^{0}(0.0007)^{2}
\]</span> <span class="math display">\[
p(\mathbf{W_{Fed51}}\mid\mu_{Hamilton}) = 0.0000000008346
\]</span> <span class="math display">\[
p(\mathbf{W_{Fed51}}\mid\mu_{Madison}) =
\frac{26!}{(23!)(1!)(0!)(0!)(2!)}(0.92)^{23}(0.001)^{1}(0.033)^{0}(0.013)^{0}(0.023)^{2}
\]</span> <span class="math display">\[
p(\mathbf{W_{Fed51}}\mid\mu_{Madison}) = 0.00055692
\]</span> <span class="math display">\[
p(\mathbf{W_{Fed51}}\mid\mu_{Jay}) =
\frac{26!}{(23!)(1!)(0!)(0!)(2!)}(0.97)^{23}(0.01)^{1}(0)^{0}(0.01)^{0}(0)^{2}
\]</span></p>
<p><span class="math display">\[
p(\mathbf{W_{Fed51}}\mid\mu_{Jay}) = 0
\]</span></p>
<p>Voilà – matching the academic consensus, our results tell us that
James Madison was the most likely author of <i>Federalist 51</i>.
However, a primary concern that may emerge is that, while the
probability of John Jay authoring is very low, our results effectively
say it is impossible because he did not record <i>whilst</i> or
<i>man</i> in any of the <i>Federalist Papers</i>. We can do better by
regularizing our estimates – i.e., adding a small positive number to
each vector to encode the possibility that (let’s say) Jay might someday
use those words.</p>
<p>For the sake of ease, I am going to replicate our results with
Laplace smoothing and <code>dmultinom()</code>, where
federalist_51_vector is the vector of word-specific counts, and the
author-specific vectors are the <span class="math inline">\(\mu\)</span>
values for Hamilton, Madison, or Jay plus 1 (such that Jay now records
at least 1 instance of each word)</p>
<pre class="r"><code>hamilton_likelihood &lt;- dmultinom(x = federalist_51_vector,
                                 prob = author_vectors[[&#39;Hamilton&#39;]] + 1)

madison_likelihood &lt;- dmultinom(x = federalist_51_vector,
                                 prob = author_vectors[[&#39;Madison&#39;]] + 1)

jay_likelihood &lt;- dmultinom(x = federalist_51_vector,
                                 prob = author_vectors[[&#39;Jay&#39;]] + 1)


data.frame(Author = c(&#39;Hamilton&#39;, &#39;Madison&#39;, &#39;Jay&#39;), 
           Likelihood = c(hamilton_likelihood, madison_likelihood, jay_likelihood))</code></pre>
<pre><code>##     Author   Likelihood
## 1 Hamilton 3.845094e-08
## 2  Madison 2.557079e-02
## 3      Jay 2.222033e-03</code></pre>
<pre class="r"><code>madison_likelihood/jay_likelihood # Likelihood Ratio of Madison v. Jay</code></pre>
<pre><code>## [1] 11.50783</code></pre>
<pre class="r"><code>madison_likelihood/hamilton_likelihood # Likelihood Ratio of Madison v. Hamilton </code></pre>
<pre><code>## [1] 665023.7</code></pre>
<p>So - Yeah, We can be <i>very</i> confident from our niche analysis
that James Madison was likely the author! But, for the sake of ease,
let’s do a robustness check using <i>Federalist 10</i> – which we know
James Madison wrote.</p>
<pre class="r"><code>  hamilton_likelihood &lt;- dmultinom(x = federalist_10_vector,
                                 prob = author_vectors[[&#39;Hamilton&#39;]] + 1)
  
  madison_likelihood &lt;- dmultinom(x = federalist_10_vector,
                                 prob = author_vectors[[&#39;Madison&#39;]] + 1)
  
  jay_likelihood &lt;- dmultinom(x = federalist_10_vector,
                                 prob = author_vectors[[&#39;Jay&#39;]] + 1)

  
  madison_likelihood/jay_likelihood # Madison vs. Jay</code></pre>
<pre><code>## [1] 18.06388</code></pre>
<pre class="r"><code>  madison_likelihood/hamilton_likelihood # Madison v. Hamilton</code></pre>
<pre><code>## [1] 181170.9</code></pre>
<p>Same confidence!</p>
<hr />
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
