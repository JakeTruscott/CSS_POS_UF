<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Multinomial Language Model</title>

<script src="multinomial_language_model_files/header-attrs-2.28/header-attrs.js"></script>
<script src="multinomial_language_model_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="multinomial_language_model_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="multinomial_language_model_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="multinomial_language_model_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="multinomial_language_model_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="multinomial_language_model_files/navigation-1.1/tabsets.js"></script>
<link href="multinomial_language_model_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="multinomial_language_model_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Multinomial Language Model</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="multinomial-language-model" class="section level2">
<h2>Multinomial Language Model</h2>
<p>The multinomial language model is a probabilistic framework used to
model the distribution of words in a document or collection of
documents. It assumes that each word in a document is drawn
independently from a fixed vocabulary according to a categorical
distribution – i.e., in accordance with the <i>Bag of Words</i>
approach, where each word has a certain probability of occurring. The
model is <i>multinomial</i> in that it considers the counts of each word
in a document, rather than just the presence or absence of words.
Formally, for a document represented as a sequence of word counts, the
likelihood of observing the document is given by the multinomial
probability mass function (PMF), which combines the factorial of the
total word count with the product of the probabilities of each word
raised to the power of its observed count. This framework forms the
basis for many text modeling techniques, including Naive Bayes
classifiers and topic models (which we will explore more this week and
next…), and provides a straightforward way to estimate the probability
of unseen documents given observed word frequencies.</p>
<hr />
<p><b>GRS</b> (Ch. 6) uses a simplified three-word vocabulary
<code>(cat, dog, fish)</code> and each document only contains a single
token (i.e., instance of a type). We are going to retains a similar
structure, but add another word to our vocabulary: <br></p>
<div style="text-align:center;">
<p><code>hamburger = (1, 0, 0, 0)</code><br></p>
<p><code>salad = (0, 1, 0, 0)</code><br></p>
<p><code>taco = (0, 0, 1, 0)</code><br></p>
<p><code>nuggets = (0, 0, 0, 1)</code><br></p>
</div>
<p>Recall that this approach accords with the Bag of Words, where words
are drawn individually and independently from a categorical
distribution, where <span class="math inline">\(W_i = \mu\)</span> –
where <span class="math inline">\(\mu\)</span> is <i>a vector containing
the probability of each individual type</i>. For this example, lets say
<span class="math inline">\(\mu =\)</span> (0.3, 0.25, 0.15, 0.3).
Meaning that the probability of each token type being drawn for any
trial is:</p>
<ul>
<li><p><span class="math inline">\(p\)</span>(<code>hamburger</code>) =
0.3</p></li>
<li><p><span class="math inline">\(p\)</span>(<code>salad</code>) =
0.25</p></li>
<li><p><span class="math inline">\(p\)</span>(<code>taco</code>) =
0.15</p></li>
<li><p><span class="math inline">\(p\)</span>(<code>nuggets</code>) =
0.3</p></li>
</ul>
<p>In other words, each word in the document is generated by
independently sampling from these four categories according to their
respective probabilities. Let’s assume we were interested in the
probability of drawing the document (<code>hamburger</code>,
<code>hamburger</code>, <code>taco</code>, <code>nuggets</code>). The
resulting count vector would be (2, 0, 1, 1) – representing 2 instances
of <code>hamburger</code>, 0 instances of <code>salad</code>, and 1
instance of both <code>taco</code> and <code>nuggets</code>.</p>
<p>Recall that the probability mass function for a categorical
distribution is:</p>
<p><span class="math display">\[
p(\mathbf{W}_i \mid \boldsymbol{\mu})
= \prod_{j=1}^J \mu_j^{w_{ij}}
\]</span></p>
<p>which we can generalize for documents that are longer than one word
using the multinomial distribution, where <span
class="math inline">\(\mathbf{M}\)</span> is an integer that controls
the number of tokens (i.e., length of the document):</p>
<p><span class="math display">\[
p(\mathbf{W}_i \mid \boldsymbol{\mu}) =
\frac{M!}{\prod_{j=1}^J W_{ij}!} \prod_{j=1}^J
\mu_{j}^{\mathbf{W}_{ij}}
\]</span></p>
<p>Supplementing our values for the hypothetical document
(<code>hamburger</code>, <code>hamburger</code>, <code>taco</code>,
<code>nuggets</code>), we get:</p>
<p><span class="math display">\[
p(\texttt{H,H,T,N} \mid \mu) =
\frac{4!}{(2_{H}!)(0_{S}!)(1_{T}!)(1_{N}!)}
(0.3_H)^2 (0.25_S)^0 (0.15_T)^1 (0.3_N)^1
\]</span></p>
<p><span class="math display">\[
= \frac{4!}{2!\cdot0!\cdot1!\cdot1!}\quad 0.09 \cdot 1 \cdot 0.15 \cdot
0.3
\]</span></p>
<p><span class="math display">\[
= 12 \cdot 0.00406 \\
\]</span> <span class="math display">\[
p(H,H,T,N \mid \mu) \approx 0.0486
\]</span></p>
<hr />
<p>As <b>GRS</b> (Ch.6) also note, the advantage of specifying a
probability model is it accompanies a set of known results that are a
consequence of the modeling assumptions – e.g., expectation, variance,
and covariance.</p>
<p><span class="math display">\[
\text{Expected Number of Times Word } j \text{ appears in Document } i
\]</span> <span class="math display">\[
E[\mathbf{W_{ij}}] = M_i\mu_j
\]</span> <br> <span class="math display">\[
\text{Variance of the count of word } j \text{ in Document } i
\]</span></p>
<p><span class="math display">\[
\text{Var}(\mathbf{W}_{ij{}}) = M_i\mu_j(1-\mu_j)
\]</span> <br> <span class="math display">\[
\text{Covariance of the count of word } j \text{ given word } k  \text{
in Document } i; j \neq k
\]</span> <span class="math display">\[
\text{Cov}(\mathbf{W}_ij, \mathbf{W}_ik) = -M_i\mu_u\mu_j
\]</span> <br></p>
<p>Putting it all together…</p>
<pre class="r"><code>mu &lt;- c(hamburger = 0.3,
        salad     = 0.25,
        taco      = 0.15,
        nuggets   = 0.3)  # Mu (Probs)

M &lt;- 4  # Document length (number of tokens)


expectation_wij &lt;- M * mu # i.e., If I repeatedly generated documents of length M from mu, this is the average number of times each word would appear.

expectation_wij</code></pre>
<pre><code>## hamburger     salad      taco   nuggets 
##       1.2       1.0       0.6       1.2</code></pre>
<pre class="r"><code>variance_wij &lt;- M * mu * (1 - mu) # i.e., How much the count of each word bounces around from document to document -- Does it show up a lot (1), never (0), or 50/50 (0.5)? 

variance_wij</code></pre>
<pre><code>## hamburger     salad      taco   nuggets 
##      0.84      0.75      0.51      0.84</code></pre>
<pre class="r"><code>covariance_wij &lt;- -M * (mu %o% mu)  # i.e., How much is the variance of one word related to another? We use off-diagonal elements -- how counts of *different* words move together (all sum to M!)


diag(covariance_wij) &lt;- variance_wij # Note: outer-product for off-diagonal gives us wrong values for diagonal (M * mu_j * mu_k) -- so we replace with true variance for W_ij 

covariance_wij</code></pre>
<pre><code>##           hamburger salad  taco nuggets
## hamburger      0.84 -0.30 -0.18   -0.36
## salad         -0.30  0.75 -0.15   -0.30
## taco          -0.18 -0.15  0.51   -0.18
## nuggets       -0.36 -0.30 -0.18    0.84</code></pre>
<hr />
<div id="example-unaccredited-federalist-papers" class="section level3">
<h3>Example: Unaccredited Federalist Papers</h3>
<p><b>GSR</b> (Ch.6) provide an illustrative example from <a
href="https://www.jstor.org/stable/pdf/2283270.pdf?casa_token=OoOmdwxe1bUAAAAA:FdpJImtxN8WZtE_qSHo_K4eWTT2QfenadrcR9DuabjyvfLVthPoesEBgQWONCeuJkJaSB3xy-nMqTI8KswcB6XrzUrbFaH-kjJrcvkjw60yF9weCsNU">Mosteller
and Wallace (1963)</a> concerning the disputed authorship of 12
Federalist Papers – a collection of 85 essays authored by Alexander
Hamilton, James Madison, and John Jay between October 1787 and August
1788 advocating for the ratification of the U.S. Constitution. Since
each Paper was authored using the same collective pseudonym –
<i>Publius</i> – authorship was disputed. Or, at least, debate was
surely had concerning which of the three – Hamilton, Madison, or Jay –
was a particular Paper’s author. Some were later discovred to have been
authored jointly. However, by the mid-20th century, it was believed that
Jay authored (5), Hamilton authored at least (43), and Madison authored
(14).</p>
<p>Mosteller and Wallace (1963) were able to use basic notions from the
<i>Bag of Words</i> and multinomial language models to infer authorship
for those disputed papers given variance in the writing styles of each
known author. In essence, given variance in the writing styles of
Hamilton, Madison, and Jay, we can infer authorship of those without
attribution.</p>
<p><i>The code below is courtesy of Dr. Joseph Ornstein (UGA)</i></p>
<pre class="r"><code>library(rvest)

page &lt;- read_html(&#39;https://www.gutenberg.org/cache/epub/18/pg18-images.html&#39;) # Recover HTML of Project Gutenberg Site

chapters &lt;- html_elements(page, &#39;.chapter&#39;) # Recover Chapter Elements

text &lt;- html_text2(chapters) # Convert Chapter to Text


federalist &lt;- tibble(text) %&gt;%
  filter(!str_detect(text, &#39;slightly different version&#39;)) %&gt;%
  mutate(
    author = str_extract(text, &#39;HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY&#39;) %&gt;%
             str_to_title(),
    title = str_extract(text, &#39;No\\. [A-Z].*&#39;)
  )

federalist_corpus &lt;- quanteda::corpus(federalist, text_field = &quot;text&quot;)

# 3. Tokenize and create DFM
federalist_tokens &lt;- quanteda::tokens(federalist_corpus)
federalist_dfm &lt;- dfm(federalist_tokens) %&gt;%
  dfm_trim(min_termfreq = 2)

interesting_words &lt;- c(&#39;although&#39;, &#39;always&#39;, 
                       &#39;commonly&#39;, &#39;consequently&#39;,
                       &#39;considerable&#39;, &#39;heretofore&#39;, 
                       &#39;upon&#39;, &#39;whilst&#39;) # Filter to Interesting Words


federalist_dfm &lt;- dfm_select(federalist_dfm, pattern = interesting_words)

# 5. Group by author
federalist_author_dfm &lt;- dfm_group(federalist_dfm, groups = federalist$author)

# 6. View top words per author (optional)
top_words_author &lt;- quanteda.textstats::textstat_frequency(federalist_author_dfm, n = 15)
top_words_author</code></pre>
<pre><code>##        feature frequency rank docfreq group
## 1         upon       387    1       5   all
## 2       always        84    2       5   all
## 3 considerable        59    3       5   all
## 4 consequently        29    4       4   all
## 5     commonly        26    5       4   all
## 6       whilst        24    6       4   all
## 7     although        17    7       4   all
## 8   heretofore        15    8       3   all</code></pre>
<pre class="r"><code># 7. Comparison wordcloud (only filtered words)
quanteda.textplots::textplot_wordcloud(federalist_author_dfm,
                                       comparison = TRUE,
                                       max_words = 100,
                                       color = c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;, &quot;orange&quot;))</code></pre>
<p><img src="multinomial_language_model_files/figure-html/federalist-1.png" width="672" /></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
