---
title: "Dictionaries"
subtitle: "POS6933: Computational Social Science"
author: "Truscott (Spring 2026)"
output:
  html_document:
    self_contained: false
    layout: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(ggplot2)
library(dplyr)
library(cowplot)
library(stargazer)
library(doParallel)
library(parallel)
library(snow)
library(tm)
library(quanteda)
library(gutenbergr)
library(stringr)

reduce_complexity <- function(text){
  text <- tolower(text) # Lower Case
  text <- tm::removePunctuation(text) # Punctuation
  text <- tm::removeNumbers(text) # Numbers
  text <- removeWords(text, tm::stopwords("english")) # Stop Words
  text <- unlist(stringr::str_split(text, '\\s+')) # Tokenize 
  text <- textstem::lemmatize_words(text) # Lemmatize
  text <- paste(text, collapse = ' ') # Re-Append
  text <- gsub("\\s{2,}", ' ', text) # 2 or More Spaces --> One Space
  text <- trimws(text) # White Space
  return(text)
}

set.seed(41)

```

------------------------------------------------------------------------

## Dictionaries

There are few intellectual exercises more demanding (or more burdensome...) than studying variation in language. Most modern English dictionaries estimate the language to contain no fewer than 170,000 words, a figure that continues to evolve as some terms fall into obsolescence while others are continually introduced (often to the dismay of those who find Generation Alpha brain rot slang less than inspiring...). Matters become even more complicated when we consult a thesaurus and recognize that nearly every word possesses multiple synonyms, each differing in appropriateness depending on meaning, tense, register, and broader syntactic context.

For the moment, we will set aside many of these complexities until we turn to vectorized word embeddings in high-dimensional spaces. For now, let us assume that words -- irrespective of tense or usage -- possess localized and singular meanings. Consider the following sentence:

$$
\text{I loved the new restaurant; the food was delicious and the service was fantastic}
$$ After reducing complexity using the same approach as <a href="https://jaketruscott.github.io/CSS_POS_UF/class_5/bag_of_words.html">our previous class</a>, we are left with <i>love new restaurant food delicious service fantastic</i>.While we might have been able to infer from just reading the original sentence that the implied <b> rhetorical sentiment</b> (i.e., the attitudinal or affective quality of text) was certainly <i>positive</i> (as opposed to <i>neutral, negative</i>, or some alternative <b>classification</b>), reducing the complexity we are left with two words (<i>glad & enjoy</i>) that still reflect a <i>positive</i> sentiment, while <i>movie</i> appears to not relay a perceivable sentiment. In its most basic form, we can simply assert that because there are (2) <i>positive</i> words and (0) <i>negative</i> words, the sentence is <i>positive</i>!

Now let's consider another sentence:

$$
\text{The movie was a disappointment; the plot was confusing and the acting was poor}
$$ Much like the first sentence, we could probably infer the <i>negative</i> intent, but removing complexity again further reinforces our belief (<i>movie disappointment plot confuse act poor</i>). The terms <i>disappointment, confuse,</i> and <i>poor</i> offer a preponderance of evidence that the sentence's implied sentiment is <i>negative</i>

But what about a sentence like this:

$$
\text{The concert had amazing visuals and energy, but the sound quality was inconsistent}
$$ Which produces <i>concert amaze visual energy sound quality inconsistent</i>. Suddenly our ability to consider sentiment at face value is a bit more challenging -- surely we could make the argument that the author certainly enjoyed the concert, but it is not a full-throated endorsement due to the inconsistent sound quality. Assessing sentiment at the word level (<i>concert amaze visual energy sound quality inconsistent</i>) now requires greater diligence -- as we could reasonably assert that although the sentence contains at least one <i>positive</i> term (<i>amaze</i>) and one <i>negative</i> term (<i>inconsistent</i>) whether and how we assess meaning to the remaining terms with debatable classifications (<i>energy & quality</i>), could have an important impact on our ability to classify this sentence correctly, as well as any others where we employ that approach.

This approach is at the core of dictionary methods -- perhaps the best introductory approach to understanding how we can derive meaning (e.g., rhetorical sentiment) from text. In short, dictionary methods rely on a pre-defined lexicon where each word (or n-gram) is assigned to one or more categories, such as <i>positive</i> or <i>negative</i> sentiment. The strength of this approach lies in having a large, carefully curated vocabulary whose classifications are both theoretically grounded and defensable (i.e., terms that you define as + or - are truly such.) By counting the frequency of words in each category, we can estimate the presence and intensity of particular meanings or sentiments across sentences, passages, and documents.

Before I demonstrate a handful of the existing dictionary methods in <code>R</code>, I want to emphasize (4) key points about dictionary classifiers (which I already touch on briefly in the previous note):

1.  Dictionary classifiers, while rather simple and intuitive, can nevertheless provide robust classification power. Especially among documents retaining authentic discursive properties -- i.e., authors refrain from literary tools like misdirection, sarcasm, or other inauthentic speech, a dictionary strategy provides a lot of upside for considerably little technical effort.

2.  However, much of that validity relies on the ability of researchers to construct dense lexicons. Much like anything in statistical analyses, small-N problems easily permeate dictionary approaches when the size of variety of the lexicon is insufficient to capture the variance of the documents themselves.

3.  Moreover, while a dense dictionary is certainly important, paramount to such is constructing categorical classifications (e.g., positive or negative) that are both authentic and defensible. For example, terms like <i>happy</i> and <i>joyous</i> are (almost always) associated with <i>positive</i> sentiments, while terms like <i>depressing</i> or <i>hateful</i> are certainly more negative. Misidentifying these terms will surely impact your ability to make valid classifications.

4.  Finally, and in a similar vein, dictionary methods are sensibly plagued by the same concern as the bag of words, more generally. In particular, words have associated meaning based on tense and conditional usage. For instance, <i>happy</i> is certainly <i>positive</i> -- but what if the sentence actually reads <i>I was not happy at all</i>. Suddenly that very negative sentence may be misclassified as a <i>positive</i> sentence based solely on the classification label for <i>happy</i>. Much of these concerns (as with other areas concerning the bag of words) can be improved given more robust methods, including the incorporation of inverses (<i>not</i>) and n-grams.

### Sentiment Analysis Using Dictionary Methods (Examples in <code>R</code>)

Below I provide a handful of sandboxed dictionary methods -- all of which will rely on the same collection of strings. In particular, I will focus just on three methods: Bing (Word $\rightarrow$ Class), Afinn (Word $\rightarrow$ Scalar), and Sentimentr (Sentiment = Polarity x Valence). 

```{r, dictionary_strings}

strings <- c('This decision is excellent, fair, and clearly the right outcome', 
             'The opinion is good and persuasive, even if it is not perfect', 
             'The ruling has some good points but also several serious flaws', 
             'The decision is bad and poorly reasoned', 
             'This opinion is terrible, deeply unfair, and completely wrong')

strings <- tibble(
  doc_id = seq_along(strings),
  text = strings) 

strings_tokens <- strings %>% # Convert to tibble
  tidytext::unnest_tokens(word, text) # Convert to Unnested Tokens

head(strings_tokens)


```

<br>

#### Bing

The **Bing** Dictionary is a sentiment lexicon widely used for binary sentiment classification. It assigns words to one of two categories (*positive* or *negative*) and contains roughly 6,800 sentiment-labeled terms (approx. 2,000 positive and 4,800 negative)

```{r, bing}

bing_dictionary <- tidytext::get_sentiments("bing")

bing_dictionary %>%
  group_by(sentiment) %>%
  slice_sample(n = 3) %>%
  ungroup() %>%
  arrange(sentiment) # Sample BING

strings_tokens %>%
  inner_join(tidytext::get_sentiments(lexicon = 'bing'), by = 'word') %>%
  count(doc_id, sentiment) %>%
  left_join(strings, by = 'doc_id') %>%
  select(text, sentiment, n)

```

<br>

#### AFINN

The **AFINN** lexicon is designed to capture **both the direction and intensity of sentiment**. Unlike binary lexicons such as Bing, AFINN assigns each word a numeric sentiment score, typically ranging from **–5 (strongly negative)** to **+5 (strongly positive)**. The dictionary contains roughly 2,500–3,400 terms.

```{r, afinn}


afinn_dictionary <- tidytext::get_sentiments("afinn") # Afinn Dictionary

afinn_dictionary %>% 
  group_by(value) %>%
  slice_sample(n = 1) %>%
  ungroup() %>%
  arrange(value) %>%
  print(n = 11) # Sample Words (Value -5 to 5)

strings_tokens %>%
  inner_join(tidytext::get_sentiments(lexicon = 'afinn'), by = 'word') %>%
  count(doc_id, value) %>%
  left_join(strings, by = 'doc_id') %>%
  select(text, value, n)


```

#### SentimentR

**SentimentR** improves on simple dictionary classifiers by explicitly modeling context and valence shifters. Rather than relying solely on raw word counts, **sentimentr** computes sentiment at the sentence level, accounting for how nearby words modify sentiment expression. At its core, it uses a BING-style polarity lexicon (Where words have a singular classification) combined with rule-based adjustments for **negators** (e.g., *not, never*, etc.), **amplifiers** (e.g., *very, extremely,* etc.), and **adversarial conjunctions** (e.g., *but*). The result is the base polarity (BING-style score) times a valence multiplier that adjusts for sentence hueristics (ex: *good* = 1x, *not good* = -1x, *barely good* = 0.5x, etc.) -- where negation doesn't invariably flip the sign, but rather it will scale the intensity. 


```{r, sentimentr}

strings %>%
  mutate(sentiment = sentimentr::sentiment_by(text)$ave_sentiment)


```
