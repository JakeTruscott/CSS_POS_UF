<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Vector Space Model</title>

<script src="vector_space_model_files/header-attrs-2.28/header-attrs.js"></script>
<script src="vector_space_model_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="vector_space_model_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="vector_space_model_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="vector_space_model_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="vector_space_model_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="vector_space_model_files/navigation-1.1/tabsets.js"></script>
<link href="vector_space_model_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="vector_space_model_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Vector Space Model</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="vector-space-model" class="section level2">
<h2>Vector Space Model</h2>
<p>While a multinomial language model looks at text as a series of words
and focuses on how likely each word is to appear, a <b>vector space
model</b> treats text as a point in space, letting us measure how
similar two texts are based on distance or direction. In essence, MLM is
all about <i>word probabilities</i> – that is, figuring out which words
are more likely in a document, whereas VSMs leverage <i>linear
algebra</i> to turn text into vectors so we can compare documents based
on their overall content, not just exact word counts.</p>
<p>As you might infer, this forms the key idea behind virtually all
similarity-based measures of text: the assumption that documents sharing
similar features – i.e., whether word frequencies or other vectorized
representations (…) are semantically alike.</p>
<p><b> GSR (Ch. 7) </b> gives a great overview using the <i>Federalist
Papers</i> example to demonstrate how we can use inner products to
articulate similar writing styles – the idea again being that greater
values represent similar documents (or authorship/writing styles). Using
our specific adjustments from the
<a href="https://jaketruscott.github.io/CSS_POS_UF/class_6/multinomial_language_model.html#multinomial-language-model">
Multinomial Language Model</a>, we can show how Madison and Hamilton’s
writing styles are similar than Madison and Jay (and by a considerable
margin):</p>
<p><span class="math display">\[\mathbf{W_{Madison}} \cdot
\mathbf{W_{Hamilton}} = (477, 1, 17, 7, 12) \cdot (861, 13, 102, 374, 1)
\]</span> <span class="math display">\[= (477 \times 861) + (1 \times
13) + (17 \times 102) + (7 \times 374) + (12 \times 1) \]</span> <span
class="math display">\[= 410,697\]</span></p>
<p><span class="math display">\[\mathbf{W_{Madison}} \cdot
\mathbf{W_{Jay}} = (477, 1, 17, 7, 12) \cdot (82, 1, 0, 1, 0, 84)
\]</span> <span class="math display">\[= (477 \times 82) + (1 \times 1)
+ (17 \times 0) + (7 \times 0) + (12 \times 84)\]</span> <span
class="math display">\[= 40,123\]</span></p>
<div id="cosine-similarity" class="section level3">
<h3>Cosine Similarity</h3>
<p>We can use a similar intuition to (again) attempt to discern the
authorship of a disputed essay. However, as <b>GSR</b> note, the
magnitude of the vectors – i.e., the volume of observations we have to
recover an accurate illustration of each authors’ word choice – might
give Hamilton a clear edge due to the sheer volume of word
co-occurrence. This raises an important question: <b>Is similarity
(here, prescribing authorship of disputed documents) best represented by
the volume of word co-occurrence, or the distribution of that
co-occurrence?</b> Put differently, should it matter more that Hamilton
used the four of the five words in our vocabulary more frequently than
Madison, or should it matter more how the distribution of that word
usage matches that in the disputed document? In a perfect world, these
are not mutually-exclusive. However, among higher-dimensional documents
with robust vocabularies, we need something that doesn’t over-inflate
large magnitudes of co-occurrence with similarity. Perhaps the most
intuitive alternative would be to normalize the vectors and assess
cosine similarity – which emphases the direction of the shared features,
rather than simply its length or volume.</p>
<p>Let’s walk through Cosine Similarity using our example that compares
Hamilton and the disputed essay. We will first compute the inner
product, compute the magnitude of those vectors, and then normalize
using cosine similarity.</p>
<p><br></p>
<p><b>Inner Product</b></p>
<p><span class="math display">\[
\mathbf{W}_{Hamilton} \cdot \mathbf{W}_{Disputed} = (861 \times 23) +
(13 \times 1) + (102 \times 0) + (374 \times 0) + (1 \times 2) \quad =
19,818
\]</span> <br> <b>Magnitude of Vectors</b></p>
<p><span class="math display">\[
||\mathbf{W}_{Hamilton}|| = \sqrt{861^2 + 13^2 + 102^2 + 374^2 + 1^2}
\quad \approx 944.33
\]</span> <span class="math display">\[
||\mathbf{W}_{Disputed}||  = \sqrt{23^2 + 1^2 + 0^2 + 0^2 + 2^2} \quad
\approx 23.10
\]</span></p>
<p><br> <b>Cosine Similarity</b></p>
<p><span class="math display">\[
cos(\mathbf{W}_{Hamilton}, \mathbf{W}_{Disputed}) = \frac{19,818}{944.33
\times 23.10} \quad \approx 0.9085
\]</span></p>
<p>Not bad – certainty more convincing that Hamilton authored than with
using MLM. But what happens when we apply the same to Madison?</p>
<p><br></p>
<p><b>Inner Product</b> <span class="math display">\[
\mathbf{W}_{Madison} \cdot \mathbf{W}_{Disputed} = (477 \times 23) + (1
\times 1) + (17 \times 0) + (7 \times 0) + (12 \times 2) \quad = 10,996
\]</span></p>
<p><br> <b>Magnitude of Vectors</b></p>
<p><span class="math display">\[
||\mathbf{W}_{Madison}|| = \sqrt{477^2 + 1^2 + 17^2 + 7^2 + 12^2} \quad
\approx 477.50
\]</span> <br> <b>Cosine Similarity</b></p>
<p><span class="math display">\[
cos(\mathbf{W}_{Madison}, \mathbf{W}_{Disputed}) = \frac{10,996}{477.50
\times 23.10} \quad \approx 0.996
\]</span> Wow! Even though cosine similarity gave much more credence to
the idea that Hamilton may have been the disputed author, normalizing
Madison’s vector as well just effectively removed any doubt that it is
indeed a Madison work. In essence, normalizing the vectors removed the
prioritization of the magnitude found in Hamilton’s vector</p>
</div>
<div id="measuring-distance" class="section level3">
<h3>Measuring Distance</h3>
<p>Another way to measure the (dis)similarity between two documents is
to imagine them as two points in space and calculating the distance
between them. This approach will be of particular importance once we
move to sentence embeddings and analyze similarity in a high-dimensional
space. For now, let’s just recognize that there are a handful of useful
measures (e.g., Euclidean and Manhattan distances) that aren’t entirely
dissimilar from subtracting the normalized cosine similarity from 1 –
i.e., Cosine <i>Distance</i>. Yet, it’s important to emphasize that
while we’re likely to get similar values – or, at least, values
representing substantively similar inferences, the choice of which
distance metric to use is generally a modeling choice, not a
mathematical one. Let’s use our values from Hamilton and the disputed
<i>Federalist Paper</i> as an example:</p>
<p><br> <b>Euclidean</b> <span class="math display">\[
||\mathbf{W}_{Hamilton}|| - ||\mathbf{W}_{Disputed}|| =
\sqrt{\sum^j_{j=1}(W_{Hamilton}-W_{Disputed})^2}
\]</span> <span class="math display">\[
||\mathbf{W}_{Hamilton}|| - ||\mathbf{W}_{Disputed}|| = \sqrt{838^2 +
12^2 + 102^2 + 374^2 + (-1)^2} \\ = \sqrt{852,669} \\\quad \approx
923.36
\]</span> <br> <b>Minkowski</b> (<span class="math inline">\(p\)</span>
=3 &amp; <span class="math inline">\(j\)</span> = size of vocabulary)
<span class="math display">\[s
d_p(W_{Hamilton}, W_{Disputed}) = (\sum^j_{j=1}|W_{Hamilton,j} -
W_{Disputed, j}|^p)^{\frac{1}{p}}
\]</span> <span class="math display">\[
d_p(W_{Hamilton}, W_{Disputed}) = (|838|^3 + |12|^3 + |102|^3 + |374|^3
+ |1|^3)^{\frac{1}{3}} \\ = (641,223,033)^{\frac{1}{3}} \\ \approx
867.86  
\]</span></p>
<p><br> <b>Manhattan</b> (Just Sum of Absolute Values because <span
class="math inline">\(p\)</span> = 1) <span class="math display">\[
||\mathbf{W}_{Hamilton}|| - ||\mathbf{W}_{Disputed}|| = |838| + |12| +
|102| + |374| + |(-1)| \\ = \quad \approx 1,327
\]</span></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
