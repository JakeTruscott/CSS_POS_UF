<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Supervised Classification</title>

<script src="supervised_learning_files/header-attrs-2.28/header-attrs.js"></script>
<script src="supervised_learning_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="supervised_learning_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="supervised_learning_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="supervised_learning_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="supervised_learning_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="supervised_learning_files/navigation-1.1/tabsets.js"></script>
<link href="supervised_learning_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="supervised_learning_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Supervised Classification</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="supervised-classification" class="section level2">
<h2>Supervised Classification</h2>
<p>Imagine you are coding party platforms or manifestos and are
interested in whether these groups are discussing social, economic, or
other policy issues. Perhaps the most tried and true method is also the
most obvious – read the documents. Completing tasks such as this with
perhaps only a few dozen (or so) documents is certainly feasible, if not
just a bit tedious. Yet, recall our payroll example from
<a href="https://jaketruscott.github.io/CSS_POS_UF/class_3/Parallel_R.html">Class
3</a> – completing the same task at scale assuming the range of
observations encompasses thousands (or millions) of pages or documents
is considerably time consuming. Moreover, more observations only
increases the propensity for manual (hand) coding to introduce bias as
the scope of language becomes more diverse.</p>
<p>However, <strong>supervised classification</strong> methods can
theoretically alleviate both obstacles (time and coding error) by
leveraging manually-coded training data to learn systematic patterns in
language and subsequently apply those patterns consistently at scale.
Rather than requiring researchers to read and classify every document,
supervised models are trained on a subset of texts that have been
labeled according to theoretically meaningful categories (e.g., social
policy, economic policy, foreign affairs, etc.). Once trained, these
models can rapidly classify thousands or even millions of documents with
a level of consistency that is difficult to achieve through human coding
alone.</p>
<p>In essence, <strong>supervised classification</strong> concerns our
ability to leverage a (small) sample of manually-classified terms,
n-grams, or documents to inform a model re: how to approach the
remaining (unseen and uncoded) observations! As you might be able to
infer, the flexibility here is quite attractive – i.e., I could just as
easily develop a <em>training set</em> (sample data of hand-coded
observations to <em>train</em> model) for classifying platforms
(manifestos) as discussing certain policy dimensions, or alternatively
whether the language they use re: particular policies is positive,
negative, liberal, conservative, etc… – the key, as will be discussed
below, is satisfying a few key assumptions regarding both our training
and <em>testing</em> (unseen) data.</p>
<div id="creating-a-training-set" class="section level3">
<h3>Creating a Training Set</h3>
<p>The first step to conducting supervised classification is to develop
a training set from the larger set of observations. The first question
I’d imagine you may ask is <em>how much – or what percentage of my
observations – need to be used for training</em>? There isn’t a
definitive answer, though I’d advise you to consider the holistic
(dis)advantages of removing observations for training – i.e., use too
few and you risk biasing your results with a non-representative training
set, while using too many serves to reduce the testing set you use for
inferences. Luckily, a great way to gauge whether you’ve found the
Goldilocks region is with <em>validation</em> – which is discussed more
below.</p>
<p>Some other (and extended) considerations (<strong>GRS</strong>):</p>
<ol style="list-style-type: decimal">
<li><p>To make sure your training set is representative, it is best to
draw from a random sample whenever possible – and perhaps even a
stratified random sample if you expect your data to be influenced by
temporal or other variances.</p></li>
<li><p>The performance of supervised learning is conditioned on the
accuracy of the training set. <strong>GRS</strong> recommends <em>at
least</em> two human coders when developing a training set so that
observations are consistently (and correctly) annotated. In
<a href="https://www.cambridge.org/core/journals/political-science-research-and-methods/article/how-to-train-your-stochastic-parrot-large-language-models-for-political-texts/8EAA8096779B5C9DDBDF1BFC71C63AEC">this
article</a>, my coauthors and I each coded the training for the
comparative classification section, compared our annotations, and
collectively resolved any discrepancies. All of this generally boils
down to a simple premise: If you say term (n-gram, document) more
accurately represents <span class="math inline">\(x\)</span> instead of
<span class="math inline">\(x&#39;\)</span>, it needs to actually
represent <span class="math inline">\(x\)</span>.</p></li>
<li><p>Generally speaking, <strong>GRS</strong> emphasize a
<em>good</em> training set should have the following characteristics:
<strong>Objective-intersubjectivity, an <em>a priori</em> design,
reliability, validity,</strong> and
<strong>replicability</strong>.</p></li>
</ol>
</div>
<div id="classifying-documents-and-checking-performance"
class="section level3">
<h3>Classifying Documents and Checking Performance</h3>
<p>The next step is selecting a classifier to learn the mapping between
the features and labels in the training set. There is a lot of
flexibility and discretion that you can exercise here, including how
features a represented, whether to remove irrelevant features (e.g.,
reduce complexity), etc. Omnce you have selected a classifier, now you
must choose a model to learn that mapping – which again is very flexible
and open to your discretion. We will discuss <em>Naive Bayes</em> in
particular with more detail later.</p>
<p>Regardless of your model selection, it is always a good rule of thumb
to conduct validation testing of the fitted model. Preferably with a
cross-validated series of observations held out from the training set,
compare the classification of those observations yielded from the fitted
model against your hand-coded labels using a <strong>confusion
matrix</strong>. Accuracy and other metrics are derived from this matrix
and serve as good evidence of the model’s general performance. Some of
these other metrics include <em>precision</em> (the proportion of
predicted <span class="math inline">\(k\)</span> classifications that
are truly <span class="math inline">\(k\)</span>) and <em>recall</em>
(the proportion of true <span class="math inline">\(k\)</span>
observations that are correctly classified by the model).</p>
<p>A question you may ask at this point is <em>what score/value is a
good indicator for validation</em>? Again, the answer here isn’t
definitive – just understand that when reporting these metrics (as you
should when aiming to publish a study that uses these methods…),
reviewers tend to approach this as a spot test. There are some reviewers
who will look at an accuracy above-50% as good (because you’ve
essentially defeated a coin flip – at least for binary classifications)
– you’ve improved the odds from random selection. Others are a bit
harder to convince and expect a classification accuracy in excess of
what we’d expect from trained coders – usually 70-80% accuracy from a
validation set.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
