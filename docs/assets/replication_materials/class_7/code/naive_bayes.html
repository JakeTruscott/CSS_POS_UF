<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Naive Bayes</title>

<script src="naive_bayes_files/header-attrs-2.28/header-attrs.js"></script>
<script src="naive_bayes_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="naive_bayes_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="naive_bayes_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="naive_bayes_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="naive_bayes_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="naive_bayes_files/navigation-1.1/tabsets.js"></script>
<link href="naive_bayes_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="naive_bayes_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Naive Bayes</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="naive-bayes" class="section level2">
<h2>Naive Bayes</h2>
<p><span class="math display">\[
P(A\mid B) = \frac{P(B\mid A) P(A)}{P(B)} \quad \text{ Bayes Rule} \\
\]</span></p>
<p><em>Naive Bayes</em> is a canonical supervised classification model
that leverages Bayes’ rule to recover the probability that an unseen
document (<span class="math inline">\(D_i\)</span>) containing words
(<span class="math inline">\(w_{i1},...,w_{ij}\)</span>) belongs to a
certain classification (<span class="math inline">\(\pi_{ik}\)</span>),
where <span class="math inline">\(\pi_{ik}\)</span> is represented as 0
or 1 given the membership of <span class="math inline">\(D_i\)</span> in
class <span class="math inline">\(k\)</span>. Using Bayes’ rule, the
posterior probability that document <span
class="math inline">\(i\)</span> belongs to class <span
class="math inline">\(k\)</span> is:</p>
<p><span class="math display">\[
p(\pi_{ik}\mid D_i) = \frac{p(\pi_{ik} = 1)p(D_i\mid \pi_{ik}
=1)}{p(D_i)} \quad \text{ Bayes&#39; Rule -- Conditional Probability} \\
p(\pi_{ik} = 1) \quad \text{ Baseline Probability that } D_i \text{
Belongs to Class } k \text{ Before Reading It} \\
p(D_i\mid \pi_{ik} = 1) = \quad \text{ Probability of Observing }
w_{i1}...w_{ij} \text{ if } D_i \text{ in Class } k \\
p(D_i) \quad \text{ Probability of Observing } D_i \text{ in Any Class
-- Will Use to Normalize At End}
\]</span> <em>Naive Assumption</em>: Conditional on class, words are
independent of each other – i.e., once you know the class of any
document, learning that one appears in it tells you nothing additional
about whether another word appears. This is almost certainty wrong
(<em>more soon</em>), but (as <strong>GRS</strong> note) baking in this
assumption makes estimation much easier and effective.</p>
<p><span class="math display">\[
W_i\mid \pi_{ik} = 1 \sim \text{ Multinomial}(\sum_jW_{ij},\mu_k) \\
p(W_i\mid \pi_{ik} = 1) \propto \prod^j_{j=1}\mu_{kj}^{W_{ij}}, \\
\hat{\mu}_{ik} = \frac{c + \sum^N_i \pi_{ik}W_{ij}}{Jc + \sum_i \sum_j
\pi_{ik}W_{ij}}
\]</span></p>
<p>There’s a lot going on here, but the intuition is effectively that
the probability of observing words <span
class="math inline">\(W_{i...j}\)</span> in class <span
class="math inline">\(k\)</span> is proportional to the vector of
probabilities for drawing each word in class <span
class="math inline">\(k\)</span>. What this means practically is that we
essentially need to collect all of the documents assigned to class <span
class="math inline">\(k\)</span> and calculate the frequency with which
word <span class="math inline">\(W_i\)</span> is used (and we can aid
problems re: sparse word usage with a similar strategy to Laplace
smoothing by adding a constant <span class="math inline">\(c\)</span>).
The final <em>Naive</em> algorithm is:</p>
<p><span class="math display">\[
p(\pi_{ik}=1\mid W_i) \propto \frac{\sum^N_i I(y_i=k)}{N} \prod^j_{j=1}
\mu_{kj}^{ij}
\]</span> Which means that the probability of document <span
class="math inline">\(i\)</span> belonging to class <span
class="math inline">\(k\)</span> is proportional to the baseline
frequency of class <span class="math inline">\(k\)</span> times the
probability of observing all the words in the document if it came from
that class, where <span class="math inline">\(\frac{\sum^N_i
I(y_i=k)}{N}\)</span> is the baseline prior and <span
class="math inline">\(\prod^j_{j=1} \mu_{kj}^{ij}\)</span> is the
likelihood term.</p>
<p>Finally, since the probabilities for single words are often very
small, the standard way to implement Naive Bayes is usually by adding an
<span class="math inline">\(arg max\)</span> term, which effectively
takes the log to convert probabilities into sums and make computation
safe and easier.</p>
<p><span class="math display">\[
\hat{y}_i = \text{arg max}_k [\text{log}p(\pi_{ik} = 1) +
\sum^J_j=1W_{ij}\text{log}\mu_{ik}]
\]</span> Which essentially just means to pick the class (<span
class="math inline">\(k\)</span>) with the largest value. In other
words, the model adds up the contributions from the prior and all the
words, then chooses the class that looks most likely — a process that
becomes much simpler and more stable when we work with logs.</p>
<div id="basic-naive-bayes-example" class="section level3">
<h3>Basic Naive Bayes Example</h3>
<p>Let’s suppose we are classifying stories from a popular newspaper and
want to know what types of content these organizations are discussing.
Let’s say we have two class – <em>Sports</em> and <em>Politics</em> –
and just three words to consider – <em>Game</em>, <em>Team</em>, and
<em>Vote</em>.</p>
<p>We first construct a training set using a random sample of the
observations, which produces:</p>
<table>
<thead>
<tr class="header">
<th align="center">Document</th>
<th align="center">Class</th>
<th align="center">Words (Counts)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">Sports</td>
<td align="center">Game (2), Team (1), Vote (0)</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">Sports</td>
<td align="center">Game (1), Team (2), Vote (0)</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">Politics</td>
<td align="center">Game (0), Team (0), Vote (3)</td>
</tr>
</tbody>
</table>
<p>The class (<span class="math inline">\(k\)</span>) priors (<span
class="math inline">\(\frac{\sum^N_i I(y_i=k)}{N}\)</span>) would be
<span class="math inline">\(p(\text{Sports}) = \frac{2}{3}\)</span> and
<span class="math inline">\(p(\text{Politics}) = \frac{1}{3}\)</span>,
because 2 of the 3 documents in the training set are labeled as
<em>Sports</em>.</p>
<p>Before we compute the word probabilities per class (<span
class="math inline">\(\mu_{kj}\)</span>) with 6 total words observed in
<em>Sports</em> and 3 in <em>Politics</em>, there are going to be terms
that don’t appear in certain classes (e.g., <em>Vote</em> does not
appear in any <em>Sports</em> documents), so we are going to add Laplace
smoothing (<span class="math inline">\(c\)</span>). With <span
class="math inline">\(c\)</span> = 1 and the total vocabulary (<span
class="math inline">\(J\)</span>) = 3, <span
class="math inline">\(\mu_{kj}\)</span> =</p>
<ul>
<li><span class="math inline">\(\mu_{Sports, Game} = \frac{3 + 1}{6 + 3}
\approx 0.444\)</span></li>
<li><span class="math inline">\(\mu_{Sports, Team} = \frac{4}{9} \approx
0.444\)</span></li>
<li><span class="math inline">\(\mu_{Sports, Vote} = \frac{1}{9} \approx
0.111\)</span></li>
<li><span class="math inline">\(\mu_{Politics, Game} = \frac{1}{6}
\approx 0.167\)</span></li>
<li><span class="math inline">\(\mu_{Politics, Team} = \frac{1}{6}
\approx 0.167\)</span></li>
<li><span class="math inline">\(\mu_{Politics, Vote} = \frac{4}{6}
\approx 0.667\)</span></li>
</ul>
<p>Assuming we introduce a new document (<span
class="math inline">\(D_{New}\)</span>) = (Game, Team, Vote):</p>
<p><span class="math display">\[
p(Sports\mid D_{New}) \propto p(Sports) \times \mu_{Sports,Game} \times
\mu_{Sports,Team}  \times  \mu_{Sports,Vote} \\
\frac{2}{3} \times 0.444 \times 0.444 \times 0.111 \approx 0.0145
\]</span> <span class="math display">\[
p(Politics\mid D_{New}) \propto p(Politics) \times \mu_{Politics,Game}
\times \mu_{Politics,Team}  \times  \mu_{Politics,Vote} \\
\frac{1}{3} \times 0.167 \times 0.167 \times 0.667 \approx 0.0062
\]</span></p>
<p>Normalizing the probabilities, we get:</p>
<p><span class="math display">\[
p(Sport|D_{New}) = \frac{0.0145}{0.0145+0.0062} \approx 0.70 \\
p(Politics|D_{New}) = \frac{0.0062}{0.0145+0.0062} \approx 0.299 \\
\]</span></p>
<p>Though the probabilities are both rather small, the document is
nevertheless more likely to be <em>Sports</em> – adding more words to
the training will certainly improve our inferential power.</p>
<p>While I haven’t done it here, we could also implement the argmax
approach by taking the log of both the priors and likelihoods, which
would preserve the preference for <em>Sports</em> while avoiding the
computational issues caused by very small probabilities (even with
Laplace smoothing).</p>
</div>
<div id="larger-example-using-r" class="section level3">
<h3>Larger Example Using R</h3>
<p>Now let’s do an extended example in <code>R</code> using a dataset of
<a href="https://tensorflow.rstudio.com/tutorials/keras/text_classification">hand-coded
IMDB movie reviews</a>.</p>
<pre class="r"><code>for (i in 1:length(imdb_files)){
  load(imdb_files[i])
} # Load Each IMDB file from Week 7 Data directory

set.seed(1234) # Set Random Seed

train_data &lt;- bind_rows(imdb_train_pos, imdb_train_neg) # Combine Train Rows 
test_data  &lt;- bind_rows(imdb_test_pos, imdb_test_neg) # combine Test Rows

train_data &lt;- train_data[sample(nrow(train_data), size = 5000, replace = FALSE), ] # Sample Out
test_data &lt;- test_data[sample(nrow(test_data), size = 1000, replace = FALSE), ] # Sample Out

train_corpus &lt;- tm::VCorpus(VectorSource(train_data$text)) # Convert Train to Corpus
test_corpus  &lt;- tm::VCorpus(VectorSource(test_data$text)) # Convert tes tto Corpus

reduce_complexity &lt;- function(corpus){
  corpus &lt;- tm_map(corpus, content_transformer(tolower))       
  corpus &lt;- tm_map(corpus, removePunctuation)
  corpus &lt;- tm_map(corpus, removeNumbers)
  corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;))
  corpus &lt;- tm_map(corpus, stripWhitespace)
  return(corpus)
} # Pre-Process (Reduce Complexity)

train_corpus &lt;- reduce_complexity(train_corpus) # Apply Complexity Reduction
test_corpus  &lt;- reduce_complexity(test_corpus) # Apply Complexity Reduction

train_dtm &lt;- removeSparseTerms(DocumentTermMatrix(train_corpus), 0.995) # Convert to DTM - Only Keep Terms in At least 0.5% of Docs
test_dtm  &lt;- DocumentTermMatrix(test_corpus, control = list(dictionary = Terms(train_dtm))) # Control Ensures Same Terms in Test as Train

train_matrix &lt;- as.data.frame(as.matrix(train_dtm)) # Convert Back to DF for NB
train_matrix$sentiment &lt;- train_data$sentiment # Ensure Sentiment 

test_matrix &lt;- as.data.frame(as.matrix(test_dtm)) # Convert to DF
test_matrix$sentiment &lt;- test_data$sentiment # Ensure Sentiment (for comparison)

head(train_matrix[,c(1:5)])</code></pre>
<pre><code>##   abandoned ability able absence absolute
## 1         0       0    1       0        0
## 2         0       0    0       0        0
## 3         0       0    0       0        0
## 4         0       0    0       0        0
## 5         0       0    0       0        0
## 6         0       1    0       0        0</code></pre>
<pre class="r"><code>nb_model &lt;- e1071::naiveBayes(sentiment ~ ., data = train_matrix) # Run NB Model

head(nb_model$tables) # Likelihood of Word Being Absent [,1] or Present [,2] Given a Class (+/-)</code></pre>
<pre><code>## $abandoned
##           abandoned
## Y                 [,1]       [,2]
##   negative 0.007278609 0.08965275
##   positive 0.007518797 0.09512492
## 
## $ability
##           ability
## Y                [,1]      [,2]
##   negative 0.01860089 0.1572729
##   positive 0.01662050 0.1278700
## 
## $able
##           able
## Y                [,1]      [,2]
##   negative 0.04407602 0.2223329
##   positive 0.06489909 0.2649762
## 
## $absence
##           absence
## Y                 [,1]       [,2]
##   negative 0.004043672 0.06347401
##   positive 0.007123071 0.08411381
## 
## $absolute
##           absolute
## Y                [,1]      [,2]
##   negative 0.01698342 0.1353509
##   positive 0.01266324 0.1153239
## 
## $absolutely
##           absolutely
## Y                [,1]      [,2]
##   negative 0.07157299 0.2818194
##   positive 0.04788287 0.2279085</code></pre>
<pre class="r"><code>predictions &lt;- predict(nb_model, newdata = test_matrix) # Make Predictions on Test Data

mean(predictions == test_matrix$sentiment) # Performance Accuracy</code></pre>
<pre><code>## [1] 0.764</code></pre>
<pre class="r"><code>table(Predicted = predictions, Actual = test_matrix$sentiment) # Confusion Matrix</code></pre>
<pre><code>##           Actual
## Predicted  negative positive
##   negative      418      143
##   positive       93      346</code></pre>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
