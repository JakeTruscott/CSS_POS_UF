<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Clustering</title>

<script src="clustering_files/header-attrs-2.28/header-attrs.js"></script>
<script src="clustering_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="clustering_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="clustering_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="clustering_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="clustering_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="clustering_files/navigation-1.1/tabsets.js"></script>
<link href="clustering_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="clustering_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Clustering</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="clustering" class="section level2">
<h2>Clustering</h2>
<p>In clustering, we assume that some latent structure – in this case, a
discrete grouping or classification – organizes the observed data. It
conditions latent group membership to explain patterns in lexical
variance, estimating the grouping that makes the observed text data most
probable. It is generally <strong>unsupervised</strong> in that, unlike
dictionary-based methods where we explicitly encode how our models
should classify certain words, clustering algorithms do not rely on
predefined labels or lexicons. Instead, they infer latent group
structure directly from patterns in the data, allowing word usage and
co-occurrence to determinate how documents are grouped.</p>
<div id="k-means-clustering" class="section level3">
<h3>K-Means Clustering</h3>
<p>As <strong>GSR</strong> (Ch. 12) note, k-means clustering is one of
the most widely used clustering algorithms. We’re not going to spend a
whole lot of time on walking through the equation(s) with example data
like we’ve done with previous topics, largely because k-means is best
understood as an iterative “rinse-and-repeat” procedure with several
researcher-driven choices regarding initialization and optimization. A
few notes:</p>
<ol style="list-style-type: decimal">
<li><p>K-means make <em>hard assignments</em> (or one-hot encodings) of
cluster assignment – i.e., each document is assigned to only one
category (for example, if we assume K = 4, only a single element of
<span class="math inline">\(\pi_i\)</span> will equal one – the rest
will be 0).</p></li>
<li><p>Importantly, k-means clustering is not something where there is
usually a single definitive answer. Rather, the goal is to minimize the
global objective function (typically the within-cluster sum of squared
distances), but there is no guarantee it will ever be recovered from any
particular iteration. Instead, the algorithm may converge to a <em>local
optimum</em> (a stable solution that satisfies the convergence criterion
but doesn’t fully minimize the function globally). This convergence
generally happens when successive iterations produce negligible changes
in cluster assignments from partitions (<span
class="math inline">\(\pi\)</span>) or cluster centers (<span
class="math inline">\(\mu\)</span>).</p></li>
<li><p>In essence, k-means clustering has an <em>assignment step</em>
that assigns each document to the closest cluster center (<span
class="math inline">\(\mu_k\)</span>) based on some distance metric
(<strong>GSR</strong> use squared Euclidean distance in Ch. 12), as well
as an <em>update step</em> that recomputes each cluster center as the
mean of all documents assigned to that cluster. It will repeat the loop
until it reaches convergence.</p></li>
</ol>
<div id="k-means-analogy" class="section level4">
<h4>K-Means Analogy</h4>
<p>Imagine you get a call from your parents that they would like help
organizing their collection of DVDs, which they would like to organize
based on similarity in style or genre. Each film has various features:
comedy, drama, and action, though most of these features aren’t mutually
exclusive (e.g., some films can be both romantic and funny).</p>
<p>To get started, we assume that there are 3 groups (comedy, drama, and
action) and place placards on the loor representing the “prime location”
for each genre. These placards correspond to the cluster centers (<span
class="math inline">\(\mu_k\)</span>) for each genre, meaning that if a
film is placed directly on that placard, it would represent the average
movie for that group (ex: placing a film directly on the <em>action</em>
placard means it represents the average action movie).</p>
<p>Next, we randomly scatter the DVDs on the floor. This sets up the
initial <em>assignment step</em>, where each movie is assigned to the
nearest cluster center (placard) based on its features. Because the
initial placement is random, many DVDs will likely be far from their
“true” cluster center.</p>
<p>Once all films are assigned, we recalculate the cluster centers. Each
placard is moved to the average position of all the films currently
assigned to that cluster. This updated position now better represents
the “average movie” for the cluster.</p>
<p>We then repeat the assignment step, reassigning each film to the
nearest updated cluster center, followed by recomputing the cluster
centers again. This iterative process continues until the assignments
and cluster centers stop changing significantly, meaning the algorithm
has converged. At convergence, each film belongs to the cluster closest
to it, and each cluster center represents the “average movie” for that
group.</p>
<p>In short: a film that is (hypothetically) 30% Comedy, 60 % Drama, and
10% Action should more readily cluster with other dramatic films because
cluster assignment is based on proximity to <span
class="math inline">\(\mu_k\)</span> – the more a film aligns with a
cluster’s center, the more likely it is to be assigned to that
cluster.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
