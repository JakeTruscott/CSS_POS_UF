<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Clustering</title>

<script src="clustering_files/header-attrs-2.28/header-attrs.js"></script>
<script src="clustering_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="clustering_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="clustering_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="clustering_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="clustering_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="clustering_files/navigation-1.1/tabsets.js"></script>
<link href="clustering_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="clustering_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Clustering</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="clustering" class="section level2">
<h2>Clustering</h2>
<p>In clustering, we assume that some latent structure – in this case, a
discrete grouping or classification – organizes the observed data. It
conditions latent group membership to explain patterns in lexical
variance, estimating the grouping that makes the observed text data most
probable. It is generally <strong>unsupervised</strong> in that, unlike
dictionary-based methods where we explicitly encode how our models
should classify certain words, clustering algorithms do not rely on
predefined labels or lexicons. Instead, they infer latent group
structure directly from patterns in the data, allowing word usage and
co-occurrence to determinate how documents are grouped.</p>
<div id="k-means-clustering" class="section level3">
<h3>K-Means Clustering</h3>
<p>As <strong>GSR</strong> (Ch. 12) note, k-means clustering is one of
the most widely used clustering algorithms. We’re not going to spend a
whole lot of time on walking through the equation(s) with example data
like we’ve done with previous topics, largely because k-means is best
understood as an iterative “rinse-and-repeat” procedure with several
researcher-driven choices regarding initialization and optimization. A
few notes:</p>
<ol style="list-style-type: decimal">
<li><p>K-means make <em>hard assignments</em> (or one-hot encodings) of
cluster assignment – i.e., each document is assigned to only one
category (for example, if we assume K = 4, only a single element of
<span class="math inline">\(\pi_i\)</span> will equal one – the rest
will be 0).</p></li>
<li><p>Importantly, k-means clustering is not something where there is
usually a single definitive answer. Rather, the goal is to minimize the
global objective function (typically the within-cluster sum of squared
distances), but there is no guarantee it will ever be recovered from any
particular iteration. Instead, the algorithm may converge to a <em>local
optimum</em> (a stable solution that satisfies the convergence criterion
but doesn’t fully minimize the function globally). This convergence
generally happens when successive iterations produce negligible changes
in cluster assignments from partitions (<span
class="math inline">\(\pi\)</span>) or cluster centers (<span
class="math inline">\(\mu\)</span>).</p></li>
<li><p>In essence, k-means clustering has an <em>assignment step</em>
that assigns each document to the closest cluster center (<span
class="math inline">\(\mu_k\)</span>) based on some distance metric
(<strong>GSR</strong> use squared Euclidean distance in Ch. 12), as well
as an <em>update step</em> that recomputes each cluster center as the
mean of all documents assigned to that cluster. All of this is in
pursuit of the <strong>objective function</strong> (minimizes the sum of
the squared distance) and will repeat the loop until
convergence.</p></li>
</ol>
<div id="k-means-analogy" class="section level4">
<h4>K-Means Analogy</h4>
<p>Imagine you get a call from your parents that they would like help
organizing their collection of DVDs, which they would like to organize
based on similarity in style or genre. Each film has various features:
comedy, drama, and action, though most of these features aren’t mutually
exclusive (e.g., some films can be both romantic and funny).</p>
<p>To get started, we assume that there are 3 groups (comedy, drama, and
action) and place placards on the loor representing the “prime location”
for each genre. These placards correspond to the cluster centers (<span
class="math inline">\(\mu_k\)</span>) for each genre, meaning that if a
film is placed directly on that placard, it would represent the average
movie for that group (ex: placing a film directly on the <em>action</em>
placard means it represents the average action movie).</p>
<p>Next, we randomly scatter the DVDs on the floor. This sets up the
initial <em>assignment step</em>, where each movie is assigned to the
nearest cluster center (placard) based on its features. Because the
initial placement is random, many DVDs will likely be far from their
“true” cluster center.</p>
<p>Once all films are assigned, we recalculate the cluster centers. Each
placard is moved to the average position of all the films currently
assigned to that cluster. This updated position now better represents
the “average movie” for the cluster.</p>
<p>We then repeat the assignment step, reassigning each film to the
nearest updated cluster center, followed by recomputing the cluster
centers again. This iterative process continues until the assignments
and cluster centers stop changing significantly, meaning the algorithm
has converged. At convergence, each film belongs to the cluster closest
to it, and each cluster center represents the “average movie” for that
group.</p>
<p>In short: a film that is (hypothetically) 30% Comedy, 60 % Drama, and
10% Action should more readily cluster with other dramatic films because
cluster assignment is based on proximity to <span
class="math inline">\(\mu_k\)</span> – the more a film aligns with a
cluster’s center, the more likely it is to be assigned to that
cluster.</p>
</div>
<div id="k-means-example" class="section level4">
<h4>K-Means Example</h4>
<p>Below are two example implementations of k-means in <code>R</code>
using the <code>iris</code> dataset (a built-in dataset of iris flowers
from three species). The only difference between the two being the
number of assigned cluster centers (<span
class="math inline">\(K\)</span> = 3 or 4). As you can see, adding
another cluster affects the objective function – i.e., the sum of
squared differences is reduced. However, I want to emphasize that we may
not want to read too much into this. Adding more clusters is going to
reduce the objective, but that doesn’t necessarily imply a better or
more meaningful solution. Especially with text data, it’s important to
be holistic in your assessment of clustering – <strong>GSR</strong> Ch.
12.4-12.5 does a great job discussing this!</p>
<pre class="r"><code>data &lt;- iris %&gt;%
  rename(sepal_length = Sepal.Length, 
         sepal_width = Sepal.Width, 
         petal_length = Petal.Length, 
         petal_width = Petal.Width) %&gt;%
  select(sepal_length, sepal_width, petal_length, petal_width)

kmeans_example &lt;- kmeans(data, # Iris Data 
                         centers = 3, # Cluster Centers (3 Defined)
                         nstart = 25) # No. Independent Runs 

kmeans_example$tot.withinss # Within-Cluster Sum of Squares (Dispersion Measure)</code></pre>
<pre><code>## [1] 78.85144</code></pre>
<p><img src="clustering_files/figure-html/kmeans_figure-1.png" width="672" /><img src="clustering_files/figure-html/kmeans_figure-2.png" width="672" /></p>
</div>
</div>
<div id="clustering-components" class="section level3">
<h3>Clustering Components</h3>
<p>As <strong>GSR</strong> note (and I imply above), clustering
algorithms are holistically a bit different than other concepts we’ve
discussed so far. Perhaps the most notable difference is that, unlike
other circumstances where we (more or less) implicitly know which models
are built to conform to certain data generating processes, clustering
algorithms like k-means offer a lot of flexibility in how you may wish
to view notions of (dis)similarity, iteration, and what a “correct”
configuration might look like.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Document (Dis)Similarity</strong>: While k-means uses the
squared Euclidean distance as a means to discern similarity (which also
implicitly defines dissimilarity…), there are several alternatives –
including cosine similarity and Manhattan distance. Interestingly, the
common practice (as you may have inferred from some of our other
readings) is to effectively benchmark different approaches and compare
their performance given a set of metrics (<em>More Next</em>). For
instance, you can compare the dispersion between documents among shared
clusters, as well as conduct qualitative checks to confirm that
words/documents grouped together among alike clusters seem reasonably
alike.</p></li>
<li><p><strong>Measure of Partition Quality</strong>: Since clustering
is unsupervised and we’re not assessing model fit for a known data
generating processes, assessing whether a cluster partition for any
iteration or series of iterations from a certain optimization strategy
is “good” is inherently relative (and comparable to other
iterations/strategies). As noted in (1) above, common diagnostics focus
on some combination of within-cluster cohesion (e.g., summing each
document’s dissimilarity from its cluster center) to test whether the
partition actually groups alike documents effectively, as well as
qualitative assessments of how documents are grouped. In short, the
latter emphasizes a smell test – i.e., *is it reasonable and defensible
that these documents should inhabit a similar cluster?” This isn’t
algebra – multiple solutions are reasonable acceptable here!</p></li>
<li><p><strong>Optimization Algorithm</strong>: Again, this is not a
situation where there is invariably one correct answer. There is a
global maximum (which is theoretically the <em>best</em> possible
configuration, but using brute force to locate such a configuration
given the ridiculous volume of possible configurations makes it very
unlikely we can run every iteration and associated configuration to
confirm it. Instead, we generally stop iterations once the difference
between iterations is minuscule or insufficient – it doesn’t mean we’ve
located the global maximum, but rather that this local optimum is
nevertheless a great solution. A good test of quality re: optimization
is to (a) change the starting values, (b) the optimization algorithm,
and/or (c) change the convergence threshold and see what changes emerge
in the clusters. If the difference in the clusters (i.e., the documents
in them or the dispersion of the cluster centers) is in the same
ballpark as a previous (series of) configuration(s) (ex: Tables 12.3 and
12.1 in <strong>GSR</strong> Ch. 12), then you’re probably in good
shape!</p></li>
</ol>
<div id="other-clustering-considerations" class="section level4">
<h4>Other Clustering Considerations</h4>
<ol style="list-style-type: decimal">
<li><p>Algorithmic v. Probabilistic: K-means is an example of an
algorithmic clustering strategy in that it assigns each document
deterministic to the nearest cluster centroid, aiming to minimize the
sum of squared distances. Alternatively, probabilistic models like
mixture models the documents as coming from a mixture of probability
distributions, where each point has some probability of belonging to
each cluster. There are tradeoffs to both (some of which are explored in
subsequent points…), but a good rule of thumb is to acknowledge that if
you want fast, interpretable, and deterministic (yes or no) clusters,
stick if algorithmic like k-means. If you want something a bit richer
with an assumption that clusters or group alignment needs some nuance or
uncertainty, probabilistic may be the way to go.</p></li>
<li><p>Soft v. Hard Partitions: K-means uses hard partitions to assign
documents to the nearest cluster centroid. In short, if each document
was represented as a vector with length <span
class="math inline">\(K\)</span>, only one element would be (1), the
rest would be (0). Alterntively, soft partitions allow for uncertainty
given probabilistic clustering. Yet, as <strong>GSR</strong> note, the
difference is less significant than it may seem. Like hard partitions,
soft (fuzzy) clustering methods align documents toward the nearest
cluster – meaning that each document is likely to have a greater
probability of aligning with one cluster than any of the
others.</p></li>
<li><p>Means v. Mediods: This basically just concerns the difference
between assessing cluster means versus cluster medians. In the sense of
k-means/mediods, its the difference between measuring (a) the location
of the average document in the cluster, or (b) the document that
minimizes the distance to all others in the cluster. Much like concerns
re: sampling populations, means are much more sensitive to outliers –
while medians (mediods) can theoretically omit the average behavior
we’re actually interested in.</p></li>
<li><p>Flat v. Hierarchical: Flat clustering methods (like k-means)
produce a single-level partition of the data into a fixed number of
clusters. Hierarchical clustering produces a nested tree of clusters,
either by successively merging individual observations or recursively
splitting the dataset. The former starts with many individual clusters
and attempts to aggregate across successive iterations, while the latter
does the opposite – it tries to split clusters through iteration. Both
offer inferential value for studying clusters at various levels of
granularity (from broad groupings to subclusters).</p></li>
</ol>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
