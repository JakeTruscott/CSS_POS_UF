<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Truscott (Spring 2026)" />


<title>Topic Models</title>

<script src="topic_models_files/header-attrs-2.28/header-attrs.js"></script>
<script src="topic_models_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="topic_models_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="topic_models_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="topic_models_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="topic_models_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="topic_models_files/navigation-1.1/tabsets.js"></script>
<link href="topic_models_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="topic_models_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Topic Models</h1>
<h3 class="subtitle">POS6933: Computational Social Science</h3>
<h4 class="author">Truscott (Spring 2026)</h4>

</div>


<hr />
<div id="topic-models" class="section level2">
<h2>Topic Models</h2>
<p>Recall that one of our first exercises using text as data was to
analyze State of the Union addresses by former presidents – particularly
as it relates to concepts like the military and the economy. To do this,
we made at least two important assumptions:</p>
<ol style="list-style-type: decimal">
<li><p>Presidents may touch on various policy areas and themes
throughout these speeches.</p></li>
<li><p>Even though they may draw from different perspectives and
contexts, we can define a word or assert a general vocabulary to
identify those thematic elements – e.g., I can use terms like
<em>navy</em>, <em>army</em>, <em>marines</em>, etc. to reasonably
identify periods where the president is discussing military
matters.</p></li>
</ol>
<p>If we were to apply a clustering algorithm to these speeches, we may
be incidentally obscuring our ability to correctly identify these
various discussion points. Even clustering algorithms that use <em>soft
partitioning</em> are likely to draw documents towards a single cluster
center. But when we have documents where themes and other distinct
literary elements are in flux throughout, we may want to instead rely on
topic models.</p>
<p><strong>Topic Models</strong> are similar to clustering methods with
an important caveat: <em>rather than assign each document to only one
cluster, topic models assign each document with proportional membership
to all categories (topics). That is, topic models suppose that each
document is a mixture across categories – a mixed membership model</em>
(GSR Ch. 13). The key benefit here is that we don’t need a single topic
to be representative an entire document on its own. For instance, in
trying to identify how presidents may discuss the economy, topic models
may help us understand this through the lens of more discrete items like
the welfare state, taxation, foreign trade or other topics within the
broader scope of discussion related to economic policy – we don’t need
to say that only one of these should be used to discuss a speech (like a
SOTU address) that touches on all of them!</p>
<div id="latent-dirichlet-allocation" class="section level3">
<h3>Latent Dirichlet Allocation</h3>
<p>Much like how k-means is the canonical clustering algorithm, Latent
Dirichlet Allocation (LDA) is the canonical topic model. First
introduced in 2003, LDA is a generative probabilistic model – meaning
that it explicitly posits a data generating process for documents where
documents are mixtures of latent topics and words are drawn conditional
on those topics. In particular, it tries to recover:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Document-Topic Distributions</strong> – where each
document is represented a distribution over (perhaps multiple) different
topics, each with a certain probability.</p></li>
<li><p><strong>Topic-Word Distributions</strong> – where each topic is
represented as a distribution over words, meaning that each topic is
defined by a set of words (again all with an associated probability of
appearing in that topic).</p></li>
</ol>
<ul>
<li>Document Index = <span class="math inline">\(i\)</span></li>
<li>Topic Index = <span class="math inline">\(K\)</span></li>
<li>Word Position In Document (<span class="math inline">\(i\)</span>) =
<span class="math inline">\(m\)</span></li>
<li>Observed Word = <span class="math inline">\(W_{im}\)</span></li>
<li>Document-Topic Weights: <span
class="math inline">\(\pi_{ik}\)</span> where <span
class="math inline">\(\sum_{K}\pi_{iK} =1\)</span> (All Sum to 1)</li>
</ul>
<p>As you might be able to infer, LDA assumes the document-topic <span
class="math inline">\(Z_{im}\)</span> and topic-word <span
class="math inline">\(W_{im}\)</span> distributions has a Dirichlet
prior and draws from a multinomial distribution, where:</p>
<p><span class="math display">\[
Z_{im} \sim \text{Multinomial}(1,\pi_i) \quad \text{Topic Indicator for
Each Word} \\
W_{im} \sim \text{Multinomial}(1, \mu{Z_{im}}) \quad \text{Observed Word
Token For Each Word Position}
\]</span> So the probability of observing a word token in a document
(<span class="math inline">\(W_{im} = j\)</span>) conditional on both
the document-topic weights (<span
class="math inline">\(\pi_{iK}\)</span>) and the topic-word
distributions (<span class="math inline">\(\mu_k\)</span>) is a weighted
average of topic-specific word probabilities, where the weights are
given by the document’s topic proportions (<span
class="math inline">\(\pi_{iK}\)</span>):</p>
<p><span class="math display">\[
p(W_{im} = j \mid \pi_i,\mu) = \sum_K\pi_{iK}\mu_{jK}
\]</span> #### LDA Example:</p>
<pre class="r"><code>library(topicmodels)
data(&quot;AssociatedPress&quot;)
dtm &lt;- AssociatedPress

lda_model &lt;- LDA(dtm, k = 5, method = &quot;Gibbs&quot;, control = list(seed = 1234)) # LDA w/ 5 Topics &amp; Gibbs Sampling 
topicmodels::terms(lda_model, 10) </code></pre>
<pre><code>##       Topic 1  Topic 2   Topic 3      Topic 4     Topic 5     
##  [1,] &quot;i&quot;      &quot;percent&quot; &quot;court&quot;      &quot;i&quot;         &quot;government&quot;
##  [2,] &quot;people&quot; &quot;million&quot; &quot;police&quot;     &quot;president&quot; &quot;soviet&quot;    
##  [3,] &quot;two&quot;    &quot;year&quot;    &quot;two&quot;        &quot;bush&quot;      &quot;united&quot;    
##  [4,] &quot;air&quot;    &quot;billion&quot; &quot;state&quot;      &quot;house&quot;     &quot;states&quot;    
##  [5,] &quot;years&quot;  &quot;new&quot;     &quot;case&quot;       &quot;new&quot;       &quot;two&quot;       
##  [6,] &quot;new&quot;    &quot;company&quot; &quot;years&quot;      &quot;committee&quot; &quot;military&quot;  
##  [7,] &quot;just&quot;   &quot;last&quot;    &quot;federal&quot;    &quot;congress&quot;  &quot;people&quot;    
##  [8,] &quot;city&quot;   &quot;market&quot;  &quot;department&quot; &quot;dukakis&quot;   &quot;police&quot;    
##  [9,] &quot;like&quot;   &quot;prices&quot;  &quot;attorney&quot;   &quot;national&quot;  &quot;union&quot;     
## [10,] &quot;first&quot;  &quot;stock&quot;   &quot;drug&quot;       &quot;campaign&quot;  &quot;officials&quot;</code></pre>
<pre class="r"><code>topic_probs &lt;- posterior(lda_model)$topics # Prob Document i Belongs to Topic K
print(round(topic_probs[c(1:10), c(1:5)], 2))</code></pre>
<pre><code>##          1    2    3    4    5
##  [1,] 0.20 0.05 0.61 0.06 0.08
##  [2,] 0.07 0.31 0.12 0.20 0.30
##  [3,] 0.29 0.10 0.44 0.08 0.08
##  [4,] 0.30 0.17 0.13 0.14 0.25
##  [5,] 0.28 0.14 0.18 0.26 0.15
##  [6,] 0.06 0.08 0.08 0.38 0.40
##  [7,] 0.22 0.30 0.21 0.11 0.16
##  [8,] 0.13 0.13 0.10 0.39 0.25
##  [9,] 0.25 0.19 0.18 0.18 0.21
## [10,] 0.18 0.13 0.11 0.27 0.30</code></pre>
<pre class="r"><code>word_probs &lt;- posterior(lda_model)$terms # Prob Word J in Topic K
head(round(word_probs[, 1:5], 5))   </code></pre>
<pre><code>##     aaron abandon abandoned abandoning abbott
## 1 0.00000 0.00000   0.00039      0e+00  0e+00
## 2 0.00000 0.00000   0.00000      0e+00  0e+00
## 3 0.00000 0.00000   0.00000      0e+00  1e-04
## 4 0.00011 0.00016   0.00001      7e-05  0e+00
## 5 0.00000 0.00000   0.00006      0e+00  0e+00</code></pre>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
