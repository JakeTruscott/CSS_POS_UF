color = c("blue", "red"), labelsize = 0.75, fixed_aspect = TRUE,
rot.per = 0.25, random.order = FALSE)
quanteda.textplots::textplot_wordcloud(president_dfm, comparison = TRUE, max_words = 100,
color = c("blue", "red"), labelsize = 0.75, fixed_aspect = TRUE,
rot.per = 0.25, random.order = FALSE, labeloffset = F)
quanteda.textplots::textplot_wordcloud(president_dfm, comparison = TRUE, max_words = 100,
color = c("blue", "red"), labelsize = 0.75, fixed_aspect = TRUE,
rot.per = 0.25, labeloffset = T)
quanteda.textplots::textplot_wordcloud(president_dfm, comparison = TRUE, max_words = 100,
color = c("blue", "red"), labelsize = 0.75, fixed_aspect = TRUE,
rot.per = 0.25, labeloffset = F)
sotu_bar_df %>%
ggplot(aes(x = frequency, y = reorder(term, frequency))) +
geom_col(fill = 'grey', colour = 'black') +
labs(x = '\nFrequency', y = 'Term\n') +
geom_vline(xintercept =  0) +
scale_x_continuous(breaks = seq(25, 150, 25)) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
legend.position = 'none',
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5))
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5))
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5),
legend.position = 'right')
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5),
legend.key.size = 2)
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5),
legend.key.size = c(1,1))
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5),
legend.key.size = unit(3,"line"))
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5),
legend.key.size = unit(1,"line"))
sotu_term_freq %>%
group_by(group) %>%
slice_max(frequency, n = 10) %>% # Take top-10 Terms
ggplot(aes(y = reorder(feature, frequency), x = frequency)) +
geom_col(aes(fill = group), colour = 'black', position = position_dodge()) +
scale_x_continuous(breaks = seq(25, 125, 25)) +
geom_vline(xintercept = 0) +
default_ggplot_theme +
theme(axis.title.x = element_blank(),
axis.text = element_text(size = 5),
axis.title = element_blank(),
legend.text = element_text(size = 5),
legend.key.size = unit(0.25,"line"))
quanteda.textplots::textplot_wordcloud(dfm_group(sotu_dfm, groups = military_speeches$president) , comparison = TRUE, max_words = 100,
color = c("blue", "red"), labelsize = 0.75, fixed_aspect = TRUE,
rot.per = 0.25, labeloffset = F)
quanteda.textplots::textplot_wordcloud(dfm_group(sotu_dfm, groups = military_speeches$president) , comparison = TRUE, max_words = 100,
color = c("blue", "red"), labelsize = 0.75, fixed_aspect = TRUE,
labeloffset = F)
library(quanteda); (quanteda.textstats); library(quanteda.textstats); library(rvest)
library(quanteda); library(quanteda.textstats); library(quanteda.textstats); library(rvest)
page <- read_html('https://www.gutenberg.org/cache/epub/18/pg18-images.html')
federalist <- tibble(html_text2(html_elements(page, '.chapter'))) %>%
filter(str_detect(text, 'slightly different version', negate = TRUE))
federalist <- tibble::tibble(html_text2(html_elements(page, '.chapter'))) %>%
filter(str_detect(text, 'slightly different version', negate = TRUE))
federalist <- tibble::tibble(html_text2(html_elements(page, '.chapter'))) %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE))
federalist <- tibble::tibble(html_text2(html_elements(page, '.chapter')))
federalist
federalist <- tibble::tibble(html_text2(html_elements(page, '.chapter'))) %>%
setNames('text')
names(federalist)
federalist <- tibble::tibble(html_text2(html_elements(page, '.chapter'))) %>%
setNames('text') %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE))
federalist <- html_text2(html_elements(page, '.chapter')) %>%
setNames('text') %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE))
page <- read_html('https://www.gutenberg.org/cache/epub/18/pg18-images.html')
# get all the chapters
chapters <- html_elements(page, '.chapter')
# get just the text from each chapter
text <- html_text2(chapters)
d <- tibble(text)
library(tibble)
page <- read_html('https://www.gutenberg.org/cache/epub/18/pg18-images.html')
# get all the chapters
chapters <- html_elements(page, '.chapter')
# get just the text from each chapter
text <- html_text2(chapters)
d <- tibble(text)
d
fed_papers <- read_html('https://www.gutenberg.org/cache/epub/18/pg18-images.html')
essays <- html_elements(fed_papers, '.chapter')
text <- html_text2(essays)
text
text <- tibble(text)
text %>%
filter(str_detect(text, 'slightly different version', negate = TRUE)) %>%
mutate(author = text %>%
str_extract('HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY') %>%
str_to_title(),
title = str_extract(text, 'No. [A-Z].*'))
text %>%
filter(str_detect(text, 'slightly different version', negate = TRUE))
text %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE))
head(tex)
head(text)
text %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE)) %>%
mutate(author = text %>%
str_extract('HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY') %>%
str_to_title(),
title = str_extract(text, 'No. [A-Z].*'))
library(dplyr); library(ggplot2); library(stringr);  library(tm);
library(quanteda); library(quanteda.textstats); library(quanteda.textstats); library(rvest)
library(dplyr); library(ggplot2); library(stringr);  library(tm); library(quanteda); library(quanteda.textstats); library(quanteda.textstats); library(rvest) library(tibble)
fed_papers <- read_html('https://www.gutenberg.org/cache/epub/18/pg18-images.html')
essays <- html_elements(fed_papers, '.chapter')
text <- html_text2(essays)
text <- tibble(text)
text %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE)) %>%
mutate(author = text %>%
str_extract('HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY') %>%
str_to_title(),
title = str_extract(text, 'No. [A-Z].*'))
federalist <- text %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE)) %>%
mutate(author = text %>%
str_extract('HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY') %>%
str_to_title(),
title = str_extract(text, 'No. [A-Z].*'))
tidy_federalist <- federalist %>%
unnest_tokens(input = 'text',
output = 'word')
tidy_federalist <- federalist %>%
tokenizers::unnest_tokens(input = 'text',
output = 'word')
tidy_federalist <- federalist %>%
tidyr::unnest_tokens(input = 'text',
output = 'word')
tidy_federalist <- federalist %>%
tidytext::unnest_tokens(input = 'text',
output = 'word')
tidy_federalist <- federalist %>%
tidytext::unnest_tokens(input = 'text',
output = 'word') # Tokenize at Word
interesting_words <- c('by', 'man', 'upon', 'heretofore', 'whilst' ) # Vocabulary
tidy_federalist_51 <- filter(tidy_federalist,
word %in% interesting_words) %>%
filter(title == 'No. LI.') # Grab Words in interesting_words from Fed 51
tidy_federalist_10 <- filter(tidy_federalist,
word %in% interesting_words) %>%
filter(title == 'No. X.') # Same from 10
tidy_federalist <- filter(tidy_federalist,
word %in% interesting_words) %>%
filter(author %in% c('Hamilton', 'Jay', 'Madison'))
tidy_federalist %>%
count(author, word) %>%
tidyr:: pivot_wider(names_from = author,values_from = n, values_fill = 0) %>%
{
wide <- .
bind_rows(
wide,
wide %>%
select(-word) %>%
summarise(across(everything(), sum)) %>%
mutate(word = "TOTAL") %>%
select(word, everything()))
} # Print Counts of Interesting Words
federalist_51_counts <- tidy_federalist_51 %>%
mutate(word = factor(word)) %>%
count(word, .drop = FALSE) %>%
setNames(c('word', 'count'))
federalist_51_vector <- rep(0, length(interesting_words))
names(federalist_51_vector) <- interesting_words
for (i in 1:nrow(federalist_51_counts)){
temp_row <- federalist_51_counts[i,]
federalist_51_vector[[as.character(temp_row$word)]] <- temp_row$count
}
author_vectors <- list()
author_tidy_federalis <- tidy_federalist %>%
count(author, word) %>%
tidyr:: pivot_wider(names_from = author,values_from = n, values_fill = 0) %>%
{
wide <- .
bind_rows(
wide,
wide %>%
select(-word) %>%
summarise(across(everything(), sum)) %>%
mutate(word = "TOTAL") %>%
select(word, everything()))
} # Print Counts of Interesting Words
author_tidy_federalist <- tidy_federalist %>%
count(author, word) %>%
tidyr:: pivot_wider(names_from = author,values_from = n, values_fill = 0) %>%
{
wide <- .
bind_rows(
wide,
wide %>%
select(-word) %>%
summarise(across(everything(), sum)) %>%
mutate(word = "TOTAL") %>%
select(word, everything()))
} # Print Counts of Interesting Words
author_tidy_federalist
federalist_10_counts <- tidy_federalist_10 %>%
mutate(word = factor(word)) %>%
count(word, .drop = FALSE) %>%
setNames(c('word', 'count'))
federalist_10_vector <- rep(0, length(interesting_words))
names(federalist_10_vector) <- interesting_words
for (i in 1:nrow(federalist_10_counts)){
temp_row <- federalist_10_counts[i,]
federalist_10_vector[[as.character(temp_row$word)]] <- temp_row$count
}
author_vectors <- list()
author_tidy_federalist <- tidy_federalist %>%
count(author, word) %>%
tidyr:: pivot_wider(names_from = author,values_from = n, values_fill = 0) %>%
{
wide <- .
bind_rows(
wide,
wide %>%
select(-word) %>%
summarise(across(everything(), sum)) %>%
mutate(word = "TOTAL") %>%
select(word, everything()))
} # Print Counts of Interesting Words
for (i in c('Hamilton', 'Madison', 'Jay')){
temp_federalist_10_vector <- rep(0, length(interesting_words))
names(temp_federalist_10_vector) <- interesting_words
temp_values <- tidy_federalist %>%
filter(author == i) %>%
count(author, word) %>%
tidyr:: pivot_wider(names_from = author,values_from = n, values_fill = 0) %>%
setNames(c('word', 'count'))
temp_vector <- temp_federalist_10_vector
temp_vector[match(temp_values$word, names(temp_vector))] <- temp_values$count
author_vectors[[as.character(i)]] <- temp_vector
}
hamilton_likelihood <- dmultinom(x = federalist_10_vector,
prob = author_vectors[['Hamilton']] + 1)
madison_likelihood <- dmultinom(x = federalist_10_vector,
prob = author_vectors[['Madison']] + 1)
jay_likelihood <- dmultinom(x = federalist_10_vector,
prob = author_vectors[['Jay']] + 1)
madison_likelihood/jay_likelihood # Madison vs. Jay
madison_likelihood/hamilton_likelihood # Madison v. Hamilton
tidy_federalist
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% stop_words$word) %>%
filter(str_detect(word, "[a-z]"))
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% stop_words$word) %>%
filter(str_detect(word, "[a-z]"))
library(stopwords)
stopwords
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% interesting_words$word) %>%
filter(str_detect(word, "[a-z]"))
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
tf_author <- tidy_federalist_clean %>%
count(author, word, sort = TRUE)
tfidf_author <- tf_author %>%
bind_tf_idf(term = word, document = author, n = n)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
library(ggplot2)
library(dplyr)
library(cowplot)
library(stargazer)
library(doParallel)
library(parallel)
library(snow)
library(tm)
library(quanteda)
library(gutenbergr)
library(stringr)
library(tidyverse)
library(tidytext)
library(rvest)
library(dplyr); library(ggplot2); library(stringr);  library(tm); library(quanteda); library(quanteda.textstats); library(quanteda.textstats); library(rvest); library(tibble); library(tidytext)
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
tf_author <- tidy_federalist_clean %>%
count(author, word, sort = TRUE)
tfidf_author <- tf_author %>%
bind_tf_idf(term = word, document = author, n = n)
top_words <- tfidf_author %>%
group_by(author) %>%
slice_max(tf_idf, n = 100) %>%  # top 50 words per author
ungroup()
top_hamilton <- top_words %>% filter(author == "Hamilton")
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$n,
vfont=c("serif","plain"))
top_hamilton
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
tidy_federalist_clean
tidy_federaist
tidy_federalist
tidy_federalist_clean <- tidy_federalist %>%
filter(word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
tf_author <- tidy_federalist_clean %>%
count(author, word, sort = TRUE)
tfidf_author <- tf_author %>%
bind_tf_idf(term = word, document = author, n = n)
top_words <- tfidf_author %>%
group_by(author) %>%
slice_max(tf_idf, n = 100) %>%  # top 50 words per author
ungroup()
top_hamilton <- top_words %>% filter(author == "Hamilton")
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$n,
vfont=c("serif","plain"))
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$tf_idf,
vfont=c("serif","plain"))
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
tidy_federalist_clean
tidy_federalist_clean <- federalist %>%
filter(!word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
federalist
federalist
names(federalist)
essays <- html_elements(fed_papers, '.chapter')
text <- html_text2(essays)
text <- tibble(text)
federalist <- text %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE)) %>%
mutate(author = text %>%
str_extract('HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY') %>%
str_to_title(),
title = str_extract(text, 'No. [A-Z].*')) # Clean & Partition
federalist
tidy_federalist <- federalist %>%
tidytext::unnest_tokens(input = 'text',
output = 'word') # Tokenize at Word
text <- html_text2(essays)
text <- tibble(text)
federalist <- text %>%
filter(stringr::str_detect(text, 'slightly different version', negate = TRUE)) %>%
mutate(author = text %>%
str_extract('HAMILTON AND MADISON|HAMILTON OR MADISON|HAMILTON|MADISON|JAY') %>%
str_to_title(),
title = str_extract(text, 'No. [A-Z].*')) # Clean & Partition
tidy_federalist <- federalist %>%
tidytext::unnest_tokens(input = 'text',
output = 'word') # Tokenize at Word
tidy_federalist_clean <- tidy_federalist %>%
filter(!word %in% interesting_words) %>%
filter(str_detect(word, "[a-z]"))
tf_author <- tidy_federalist_clean %>%
count(author, word, sort = TRUE)
tfidf_author <- tf_author %>%
bind_tf_idf(term = word, document = author, n = n)
top_words <- tfidf_author %>%
group_by(author) %>%
slice_max(tf_idf, n = 100) %>%  # top 50 words per author
ungroup()
top_hamilton <- top_words %>% filter(author == "Hamilton")
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$n,
vfont=c("serif","plain"))
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$n,
vfont=c("serif","plain")) # No TF-IDF
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$tf_idf,
vfont=c("serif","plain"))
warnings()
top_words
top_words <- tfidf_author %>%
group_by(author) %>%
slice_max(tf_idf, n = 100) %>%  # top 50 words per author
ungroup()
top_words <- tfidf_author %>%
group_by(author) %>%
slice_max(tf_idf, n = 50) %>%  # top 50 words per author
ungroup()
top_hamilton <- top_words %>% filter(author == "Hamilton")
top_hamilton
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$n,
vfont=c("serif","plain")) # No TF-IDF
wordcloud::wordcloud(words = top_hamilton$word,
freq = top_hamilton$tf_idf,
vfont=c("serif","plain")) # TF-IDF
library(dplyr); library(ggplot2); library(cowplot); library(formatR); library(stargazer); library(parallel); library(doParallel); library(tm); library(stringr); library(quanteda); library(quanteda.textstats); library(quanteda.textstats); library(dplyr); library(ggplot2); library(stringr);  library(tm); library(quanteda); library(quanteda.textstats); library(quanteda.textstats); library(rvest); library(tibble); library(tidytext)
knitr::opts_chunk$set(
warning = FALSE,
fig.align = "center",
comment = NA,
dev = "pdf",
tidy.opts=list(width.cutoff=50),
R.options = list(width = 120)
)
default_ggplot_theme  <- theme_minimal(base_size = 12) +
theme(
plot.title = element_text(hjust = 0.5, size = 12),
axis.title = element_text(size = 12, colour = 'black'),
axis.text = element_text(size = 10, colour = 'black'),
panel.background = element_rect(linewidth = 1, colour = 'black', fill = NA),
legend.position = 'bottom',
legend.title = element_blank(),
legend.background = element_rect(linewidth = 1, colour = 'black', fill = NA),
)
reduce_complexity <- function(text){
text <- tolower(text) # Lower Case
text <- tm::removePunctuation(text) # Punctuation
text <- tm::removeNumbers(text) # Numbers
text <- tm::removeWords(text, tm::stopwords("english")) # Stop Words
text <- unlist(stringr::str_split(text, '\\s+')) # Tokenize
text <- textstem::lemmatize_words(text) # Lemmatize
text <- paste(text, collapse = ' ') # Re-Append
text <- gsub("\\s{2,}", ' ', text) # 2 or More Spaces --> One Space
text <- trimws(text) # White Space
return(text)
} # Function to Process Text for Bag of Words
set.seed(123)
afinn_dictionary <- tidytext::get_sentiments("afinn") # Afinn Dictionary
afinn_dictionary %>%
group_by(value) %>%
slice_sample(n = 1) %>%
ungroup() %>%
arrange(value) %>%
select(word) %>%
pull(word)
afinn_dictionary %>%
group_by(value) %>%
slice_sample(n = 1) %>%
ungroup() %>%
arrange(value) %>%
select(value, word) %>%
{ setNames(.$word, .$value) }
gc()
