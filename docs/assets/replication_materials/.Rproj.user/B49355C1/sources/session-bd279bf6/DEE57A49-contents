---
title: "Multinomial Language Model"
subtitle: "POS6933: Computational Social Science"
author: "Truscott (Spring 2026)"
output:
  html_document:
    self_contained: false
    layout: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(ggplot2)
library(dplyr)
library(cowplot)
library(stargazer)
library(doParallel)
library(parallel)
library(snow)
library(tm)
library(quanteda)
library(gutenbergr)
library(stringr)

```

------------------------------------------------------------------------

## Multinomial Language Model

The multinomial language model is a probabilistic framework used to model the distribution of words in a document or collection of documents. It assumes that each word in a document is drawn independently from a fixed vocabulary according to a categorical distribution -- i.e., in accordance with the <i>Bag of Words</i>, where each word has a certain probability of occurring. The model is ``multinomial'' in that it considers the counts of each word in a document, rather than just the presence or absence of words. Formally, for a document represented as a sequence of word counts, the likelihood of observing the document is given by the multinomial probability mass function (PMF), which combines the factorial of the total word count with the product of the probabilities of each word raised to the power of its observed count. This framework forms the basis for many text modeling techniques, including Naive Bayes classifiers and topic models (which we will explore more this week and next...), and provides a straightforward way to estimate the probability of unseen documents given observed word frequencies.

---

<b>GRS</b> (Ch. 6) uses a simplified three-word vocabulary <code>(cat, dog, fish)</code> and each document only contains a single token (i.e., instance of a type). We are going to retains a similar structure, but add another word to our vocabulary: <br>

::: {style="text-align:center;"}
<code>hamburger = (1, 0, 0, 0)</code><br>

<code>salad = (0, 1, 0, 0)</code><br>

<code>taco = (0, 0, 1, 0)</code><br>

<code>nuggets = (0, 0, 0, 1)</code><br>
:::

Recall that this approach accords with the Bag of Words, where words are drawn individually and independently from a categorical distribution, where $W_i = \mu$ -- where $\mu$ is <i>a vector containing the probability of each individual type</i>. For this example, lets say $\mu =$ (0.3, 0.25, 0.15, 0.3). Meaning that the probability of each token type being drawn for any trial is:

-   $p$(<code>hamburger</code>) = 0.3

-   $p$(<code>salad</code>) = 0.25

-   $p$(<code>taco</code>) = 0.15

-   $p$(<code>nuggets</code>) = 0.3

In other words, each word in the document is generated by independently sampling from these four categories according to their respective probabilities. Let's assume we were interested in the probability of drawing the document (<code>hamburger</code>, <code>hamburger</code>, <code>taco</code>, <code>nuggets</code>). The resulting count vector would be (2, 0, 1, 1) -- representing 2 instances of <code>hamburger</code>, 0 instances of <code>salad</code>, and 1 instance of both <code>taco</code> and <code>nuggets</code>.

Recall that the probability mass function for a categorical distribution is:

$$
p(\mathbf{W}_i \mid \boldsymbol{\mu})
= \prod_{j=1}^J \mu_j^{w_{ij}}
$$

which we can generalize for documents that are longer than one word using the multinomial distribution, where $\mathbf{M}$ is an integer that controls the number of tokens (i.e., length of the document): 

$$
p(\mathbf{W}_i \mid \boldsymbol{\mu}) = 
\frac{M!}{\prod_{j=1}^J W_{ij}!} \prod_{j=1}^J
\mu_{j}^{\mathbf{W}_{ij}}
$$

Supplementing our values for the hypothetical document (<code>hamburger</code>, <code>hamburger</code>, <code>taco</code>, <code>nuggets</code>), we get:


$$
p(\texttt{H,H,T,N} \mid \mu) = 
\frac{4!}{(2_{H}!)(0_{S}!)(1_{T}!)(1_{N}!)}
(0.3_H)^2 (0.25_S)^0 (0.15_T)^1 (0.3_N)^1
$$

$$
= \frac{4!}{2!\cdot0!\cdot1!\cdot1!}\quad 0.09 \cdot 1 \cdot 0.15 \cdot 0.3
$$

$$
= 12 \cdot 0.00406 \\
$$
$$
p(H,H,T,N \mid \mu) \approx 0.0486
$$
