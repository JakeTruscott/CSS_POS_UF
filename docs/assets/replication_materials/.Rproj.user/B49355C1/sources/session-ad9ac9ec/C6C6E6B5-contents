---
title: "Parallel Computing in R"
subtitle: "POS6933: Computational Social Science"
author: "Truscott (Spring 2026)"
output:
  html_document:
    self_contained: false
    layout: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(dplyr); library(ggplot2); library(ggtext); library(cowplot)

```

---

## Parallel Computing

<b>Parallel Computing</b> concerns the practice of dividing a large computational task into smaller parts that can be executed simultaneously -- that is, in <i>parallel</i> -- rather than one after another. The goal is to increase processing speeds, handle larger problems (tasks and data), and deploy resources more efficiently. 

<img src="https://raw.githubusercontent.com/JakeTruscott/CSS_POS_UF/ecfae9a49f6b7e5fb29b34ba65c8ae2bc4fc86c4/docs/assets/replication_materials/class_3/supplemental_materials/parallel_vs_serial_computing.png" 
     alt="Parallel Computing" 
     width="500" 
     style="display: block; margin: 0 auto;">


In traditional (or <b>serial</b>) computing, a program runs on a single processor core and executes one instruction at a time. As problems get larger or more complex (e.g., simulating electoral outcomes, training machine learning models, or otherwise processing massive datasets) this approach becomes too slow. Parallel computing solves this by spreading the work across multiple cores, processors, or even machines, allowing many operations to occur at once.

---

### Payroll at the University of Florida (Example)

Imagine you are responsible for discharging payroll to all employees (faculty, staff, etc.) at the University of Florida. This is approximately 33,000 people -- all of whom we can assume are registered for direct deposit and would very much appreciate being paid correctly and in a timely manner. For the sake of the example, let's further assume everything with respect to payroll can be automated. However, you are responsible with designing a programming routine to actually discharge payroll -- each of which requires 0.25 seconds. Using a single system, we can easily compute the completion time as: 

<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>33,000 (Tasks)</mn><mo>&#xD7;</mo><mn>0.25 (Seconds/Task)</mn><mo>=</mo><mn>8,250 Seconds  (2.29 Hours)</mn></math>. 


Clearly not very efficient -- but it's the result of practical limitations. Traditional computers are designed to execute instructions one after another in a sequence, which can make completing large volumes of tasks a burdensome and time-consuming endeavor. 

But what if we have <b>two</b> computers? The time to complete an independent task remains fixed (0.25 seconds/each) due to the serial constraints of the computing process, but what if we divided and allocated the tasks equally to each computer? Suddenly, the completion time has been cut in half:

<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mn>33,000 (Tasks)</mn><mo>&#xD7;</mo><mn>0.25 (Seconds/Completion)</mn></mrow><mn>2 (Computers)</mn></mfrac><mo>=</mo><mn>4,125 Seconds (1.15 Hours)</mn></mrow></math>. 

Adding additional computers -- say, 3 (About 45 Minutes), 4 (About 34 Minutes), or 5 (About 27 minutes) computers -- continues to reduce the computation time. What was once a 2.3 hour task is suddenly about 30 minutes without changing the serial structure of the computing process -- we just added more computers to handle the workload! 

This concept is at the heart of parallel computing (generally) and high performance computing environments like <code>HiPerGator</code>. Barring quantum computing (which, please... don't ask me about), computers can only operationalize tasks serially, but dividing the tasks across multiple computing units simultaneously can drastically improve computational efficiency. Even cooler, virtually every modern laptop on the consumer market can facilitate a parallel environment -- you just need to tell your computer (or <code>R</code>) to do it! 

Below, I provide some key terms to understand re: parallel computing, as well as how to set-up and deploy a parallel environment in <code>R</code>

---

## Key Terms

- <b>CPU (Central Processing Unit)</b>: The CPU is the <i>brain</i> of a computer. It performs calculations and executes instructions. Most modern CPUs have multiple cores, and each core can handle its own stream of instructions. This means a CPU can run several tasks in parallel. For example, a quad-core processor can process four tasks at once.

- <b>Processor vs. Core</b>: A processor is the entire chip that fits into your computer's motherboard (i.e., the entire CPU). A core is an individual processing unit within that chip. So, a single processor can contain multiple cores -- each capable of independent computation

- <b>Thread</b>: A thread is the smallest sequence of instructions that a CPU can execute independently. Modern operating systems can schedule multiple threads per core, allowing one core to handle multiple tasks "concurrently" by quickly switching between them. This is sometimes called <b>multithreading</b> (e.g., in the UF payroll example, one thread could calculate salaries, another could generate direct deposit files, and another could send notifications -- all at the same time on a multithreaded system.)

- <b>RAM (Random Access Memory)</b>: RAM is the computer's short-term memory. It stores data that's actively being used so the CPU can access it quickly. In parallel computing, enough RAM is crucial because multiple processors may need to access or share large amounts of data at the same time -- alternatively, allocating memory independently to an individual core (or cores) helps ensure that RAM is not accidentally overburdened trying to hold onto (unnecessary) data for sevral tasks simultaneously. 

- <b>Nodes</b>: A node is a single computer in a cluster -- a group of computers linked together to work on a common task. Each node has its own CPU(s), memory (RAM), and sometimes storage. In large-scale parallel computing (like on supercomputers), many nodes work together, connected through a high-speed network.

- <b>Cluster</b>: A cluster is a collection of nodes that work together like one powerful machine. Each node handles a portion of the computation, and the results are combined at the end. <code>HiPerGator</code> is a giant computing cluster from which we can isolate "smaller" allocations of nodes and memory for complex computing tasks. 


## Setting Up a Parallel Environment in <code>R</code>
