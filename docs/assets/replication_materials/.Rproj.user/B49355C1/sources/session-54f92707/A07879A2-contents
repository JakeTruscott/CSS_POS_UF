---
title: "Vector Space Model"
subtitle: "POS6933: Computational Social Science"
author: "Truscott (Spring 2026)"
output:
  html_document:
    self_contained: false
    layout: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(ggplot2)
library(dplyr)
library(cowplot)
library(stargazer)
library(doParallel)
library(parallel)
library(snow)
library(tm)
library(quanteda)
library(gutenbergr)
library(stringr)

```

------------------------------------------------------------------------

## Vector Space Model

While a multinomial language model looks at text as a series of words and focuses on how likely each word is to appear, a <b>vector space model</b> treats text as a point in space, letting us measure how similar two texts are based on distance or direction. In essence, MLM is all about <i>word probabilities</i> -- that is, figuring out which words are more likely in a document, whereas VSMs leverage <i>linear algebra</i> to turn text into vectors so we can compare documents based on their overall content, not just exact word counts. 

As you might infer, this forms the key idea behind virtually all similarity-based measures of text: the assumption that documents sharing similar features -- i.e., whether word frequencies or other vectorized representations (...) are semantically alike. 

<b> GSR (Ch. 7) </b> gives a great overview using the <i>Federalist Papers</i> example to demonstrate how we can use inner products to articulate similar writing styles -- the idea again being that greater values represent similar documents (or authorship/writing styles). Using our specific adjustments from the <a href="https://jaketruscott.github.io/CSS_POS_UF/class_6/multinomial_language_model.html#multinomial-language-model">  Multinomial Language Model</a>, we can show how Madison and Hamilton's writing styles are similar than Madison and Jay (and by a considerable margin): 


$$\mathbf{W_{Madison}} \cdot \mathbf{W_{Hamilton}} = (477, 1, 17, 7, 12) \cdot (861, 13, 102, 374, 1) $$
$$= (477 \times 861) + (1 \times 13) + (17 \times 102) + (7 \times 374) + (12 \times 1) $$
$$= 410,697$$



$$\mathbf{W_{Madison}} \cdot \mathbf{W_{Jay}} = (477, 1, 17, 7, 12) \cdot (82, 1, 0, 1, 0, 84) $$
$$= (477 \times 82) + (1 \times 1) + (17 \times 0) + (7 \times 0) + (12 \times 84)$$
$$= 40,123$$

### Cosine Similarity

We can use a similar intuition to (again) attempt to discern the authorship of a disputed essay. However, as <b>GSR</b> note, the magnitude of the vectors -- i.e., the volume of observations we have to recover an accurate illustration of each authors' word choice -- might give Hamilton a clear edge due to the sheer volume of word co-occurrence. This raises an important question: <b>Is similarity (here, prescribing authorship of disputed documents) best represented by the volume of word co-occurrence, or the distribution of that co-occurrence?</b> Put differently, should it matter more that Hamilton used the four of the five words in our vocabulary more frequently than Madison, or should it matter more how the distribution of that word usage matches that in the disputed document? In a perfect world, these are not mutually-exclusive. However, among higher-dimensional documents with robust vocabularies, we need something that doesn't over-inflate large magnitudes of co-occurrence with similarity. Perhaps the most intuitive alternative would be to normalize the vectors and assess cosine similarity -- which emphases the direction of the shared features, rather than simply its length or volume. 

Let's walk through Cosine Similarity using our example that compares Hamilton and the disputed essay. We will first compute the inner product, compute the magnitude of those vectors, and then normalize using cosine similarity. 

<br>

<b>Inner Product</b>

$$
\mathbf{W}_{Hamilton} \cdot \mathbf{W}_{Disputed} = (861 \times 23) + (13 \times 1) + (102 \times 0) + (374 \times 0) + (1 \times 2) \quad = 19,818 
$$
<br>
<b>Magnitude of Vectors</b>

$$
||\mathbf{W}_{Hamilton}|| = \sqrt{861^2 + 13^2 + 102^2 + 374^2 + 1^2} \quad \approx 944.33
$$
$$
||\mathbf{W}_{Disputed}||  = \sqrt{23^2 + 1^2 + 0^2 + 0^2 + 2^2} \quad \approx 23.10
$$

<br>
<b>Cosine Similarity</b>

$$
cos(\mathbf{W}_{Hamilton}, \mathbf{W}_{Disputed}) = \frac{19,818}{944.33 \times 23.10} \quad \approx 0.9085
$$

Not bad -- certainty more convincing that Hamilton authored than with using MLM. But what happens when we apply the same to Madison? 

<br>

<b>Inner Product</b>
$$
\mathbf{W}_{Madison} \cdot \mathbf{W}_{Disputed} = (477 \times 23) + (1 \times 1) + (17 \times 0) + (7 \times 0) + (12 \times 2) \quad = 10,996 
$$

<br>
<b>Magnitude of Vectors</b>

$$
||\mathbf{W}_{Madison}|| = \sqrt{477^2 + 1^2 + 17^2 + 7^2 + 12^2} \quad \approx 477.50
$$
<br>
<b>Cosine Similarity</b>

$$
cos(\mathbf{W}_{Madison}, \mathbf{W}_{Disputed}) = \frac{10,996}{477.50 \times 23.10} \quad \approx 0.996
$$
Wow! Even though cosine similarity gave much more credence to the idea that Hamilton may have been the disputed author, normalizing Madison's vector as well  just effectively removed any doubt that it is indeed a Madison work. In essence, normalizing the vectors removed the prioritization of the magnitude found in Hamilton's vector


### Measuring Distance

Another way to measure the (dis)similarity between two documents is to imagine them as two points in space and calculating the distance between them. This approach will be of particular importance once we move to sentence embeddings and analyze similarity in a high-dimensional space. For now, let's just recognize that there are a handful of useful measures (e.g., Euclidean and Manhattan distances) that aren't entirely dissimilar from subtracting the normalized cosine similarity from 1 -- i.e., Cosine <i>Distance</i>. Yet, it's important to emphasize that while we're likely to get similar values -- or, at least, values representing substantively similar inferences, the choice of which distance metric to use is generally a modeling choice, not a mathematical one. Let's use our values from Hamilton and the disputed <i>Federalist Paper</i> as an example: 

<br>
<b>Euclidean</b>
$$
||\mathbf{W}_{Hamilton}|| - ||\mathbf{W}_{Disputed}|| = \sqrt{\sum^j_{j=1}(W_{Hamilton}-W_{Disputed})^2}
$$
$$
||\mathbf{W}_{Hamilton}|| - ||\mathbf{W}_{Disputed}|| = \sqrt{838^2 + 12^2 + 102^2 + 374^2 + (-1)^2} \\ = \sqrt{852,669} \\\quad \approx 923.36
$$
<br>
<b>Minkowski</b> ($p$ =3 \& $j$ = size of vocabulary)
$$s
d_p(W_{Hamilton}, W_{Disputed}) = (\sum^j_{j=1}|W_{Hamilton,j} - W_{Disputed, j}|^p)^{\frac{1}{p}}
$$
$$
d_p(W_{Hamilton}, W_{Disputed}) = (|838|^3 + |12|^3 + |102|^3 + |374|^3 + |1|^3)^{\frac{1}{3}} \\ = (641,223,033)^{\frac{1}{3}} \\ \approx 867.86  
$$


<br>
<b>Manhattan</b> (Just Sum of Absolute Values because $p$ = 1)
$$
||\mathbf{W}_{Hamilton}|| - ||\mathbf{W}_{Disputed}|| = |838| + |12| + |102| + |374| + |(-1)| \\ = \quad \approx 1,327
$$
