---
title: "Introduction to Text Analysis"
author: "Jake S. Truscott, Ph.D"
subtitle: |
  POS6933: Computational Social Science
institute: |
  \vspace{-5mm} University of Florida \newline
  Spring 2026 \newline \newline \newline
  \includegraphics[width=3cm]{../../../beamer_style/UF.png} \quad 
  \includegraphics[width=3.1cm]{../../../images/CSS_POLS_UF_Logo.png}
output: 
  beamer_presentation:
    slide_level: 3
    keep_tex: true
    toc: false
classoption: aspectratio=169  
header-includes:
  - \PassOptionsToClass{aspectratio=169}{beamer}
  - \usepackage{../../../beamer_style/beamer_style}
  - \setbeamersize{text margin left=3.5mm,text margin right=3.5mm}
---

```{r setup, include=FALSE}
library(dplyr); library(ggplot2); library(cowplot); library(formatR); library(stargazer); library(parallel); library(doParallel)

knitr::opts_chunk$set(
  warning = FALSE, 
	fig.align = "center",
	comment = NA,
	dev = "pdf",
	size = "tiny",
	tidy = TRUE, 
  tidy.opts=list(width.cutoff=50)
)

default_ggplot_theme  <- theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 12),
    axis.title = element_text(size = 12, colour = 'black'),
    axis.text = element_text(size = 10, colour = 'black'),
    panel.background = element_rect(linewidth = 1, colour = 'black', fill = NA), 
    legend.position = 'bottom', 
    legend.background = element_rect(linewidth = 1, colour = 'black', fill = NA), 
  )

```

# Overview
\begin{itemize}
\item Non-Traditional Data Structures
\item Using Text in \texttt{R}
\item Retrieving Text Data
\item Creating a Corpus
\end{itemize}

# Non-Traditional Data Sources

### Motivation
\begin{itemize}
\item Many important political and social phenomena are expressed in language, not numbers \par \vspace{2.5mm}
\item Votes, surveys, and roll calls capture outcomes, but not always reasoning, framing, or strategy \par \vspace{2.5mm}
\item Text (in particular) allows us to observe (among other things): 
\begin{itemize}
  \item Preferences before choices
  \item Strategical signaling
  \item Agenda setting \& framing
\end{itemize} \par \vspace{2.5mm}
\item Operative Goal of Social Science: \textbf{Represent sophisticated human behaviors quantitatively} -- Viewing text as data is a similar vein.
\end{itemize}

### Defining \textit{Non-Traditional Data}
\begin{itemize}
\item \textbf{Unstructured or Semi-Structured Data} -- i.e., data that do not come in a fixed or rigid (numeric) format. Instead consist of free-form content where structure must be inferred rather than assumed.  \par \vspace{2.5mm}
\item Examples: 
\begin{itemize}
  \item Speeches, Debates, and Oral Arguments
  \item Judicial Opinions (and Separate Opinions)
  \item News Articles \& Editorials
  \item Social Media Posts
  \item Legislative Text \& Statutes
  \item Manifestos \& Party Platforms
  \item Books, Academic Articles, and Manuscripts
\end{itemize}
\end{itemize}

### What Can We Derive from Text? 
\begin{itemize}
\item Can be Used to Measure: 
\begin{itemize} 
\item Ideology
\item Sentiment, Tone, or Feeling
\item Issue or Topic Attention
\item Similarity or Coordination
\end{itemize} \par \vspace{2.5mm}
\item \textbf{Key Assumption}: Language reflects latent traits -- i.e., it represents either unseen or undefined characteristics, much in the same way that we can prescribe a series of votes in Congress as reflecting conservative or liberal tendencies. 
\end{itemize}

### Advantages and Shortcomings
\begin{columns}[t]
\begin{column}[t]{0.5\textwidth}
\textbf{Advantages}
\begin{itemize}
\item Captures nuance and context very well 
\item Methodologies are often very flexible
\item Provides for qualitative inferences and ex ante assessment 
\item Scales to large corpora and across time, actors, institutions, etc. 
\item Can be abundant where numerical data is sparse
\end{itemize}
\vfill
\end{column}
\begin{column}[t]{0.5\textwidth}
\textbf{Shortcomings}
\begin{itemize}
\item Measurement depends on modeling choices
\item Technical complexity ranges considerably -- tradeoff motivated by a priori expectations and ability to capture high dimensional relationships
\item Often introduces high dimensionality -- risks of overfitting, drawing inferrential value from spurious relationships, etc.
\item Generally requires validation against known qualities when used for measurement
\end{itemize}
\vfill
\end{column}
\end{columns}


### Key Steps

\begin{itemize}
\item Retrieving Text (\textbf{Today})
\item Pre-processing \& Reducing Feature Complexity (\textbf{Today})
\item Creating a Corpus (\textbf{Today})
\item Modeling Text 
\item Analysis
\end{itemize}



# Using Text in R

### R Data Types

\begin{itemize}
\item \texttt{numeric} -- (1, 2, 3, 4, 5)
\item \texttt{integer} -- (1L, 50L, 100L)
\item \texttt{complex} -- (9+3i)
\item \texttt{character} -- ('text strings') 
\item \texttt{logical} -- (TRUE or FALSE)
\end{itemize}

### Using Text in R 

\begin{itemize}
\item \texttt{R} (and \texttt{Python}) are both very flexible for handling \texttt{character} (\texttt{string}) data \par \vspace{2.5mm}
\item Countless sources of text data – from a single haiku to bounded volumes providing an expansive anthology of human knowledge, we can use text  analysis tools to bridge an entire domain of qualitative and quantitative inquiry.
\end{itemize}

### Text Object in R
\scriptsize
```{r, text_in_R_examples}

sample_text <- 'This is Sample Text'
print(sample_text)

sample_vector <- c('Sample 1', 'Sample 2', 'Sample 3')
print(sample_vector)

```

### Regular Expressions
\begin{itemize}
\item A regular expression (\textbf{regex}) is a pattern used to search, match, or manipulate text
\item In essence, it is compact language for telling \texttt{R} what text should look like, not what it should be exactly.
\item We will use these in conjunction with functions like \texttt{grep}, \texttt{grepl}, and \texttt{gsub} to retrieve and manipulate text. 
\end{itemize}


### Regex Examples
\scriptsize

```{r, regex_example}
library(stringr)
text <- 'This is a sample string with a date 2026-02-06, a time 14:30, an email test.user@example.com, and the number 42.'

unlist(stringr::str_extract_all(text, "\\b[a-zA-Z]+\\b")) # All Text

unlist(stringr::str_extract_all(text, "\\b\\d+\\b")) # All Numbers


```
### Regex Examples (Cont.)

```{r regex_example_cont, echo=FALSE}
text <- "This is a sample string with a date 2026-02-06, a time 14:30, an email test.user@example.com, and the number 42."
```
\scriptsize
```{r regex_example_cont_2}
unlist(stringr::str_extract(text, "\\b\\d{4}-\\d{2}-\\d{2}\\b")) # Grab Date

unlist(stringr::str_extract(text, "\\b\\d{2}:\\d{2}\\b")) # Grab Time HH:MM

unlist(stringr::str_extract(text,"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}")) # Email Address

unlist(stringr::str_extract(text, "^[^,]+")) # All Before 1st Comma

unlist(stringr::str_replace_all(text, "[[:punct:]]", "")) # Remove Punctuation

```

### Important Functions w/ Regular Expressions
\scriptsize
```{r regex_functions, echo=T}

set.seed(1234)

string <- 'The quick brown fox jumps over the lazy dog'

gsub('quick', 'wild', string) # Replace Quick

grepl('quick brown', string, ignore.case = F) # Check String 


```


### Partitioning Text (Lorem Ipsum)
\scriptsize
```{r, text_partition, tidy=FALSE, echo=T, results='asis'}

lorem_ipsum <- "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas scelerisque eros nec libero luctus, a gravida augue dictum. Integer sem est, malesuada nec mi ut, venenatis pellentesque massa. Mauris ac ex odio. Integer eget est lacus. Ut varius, sapien nec efficitur malesuada, sem lectus aliquet lacus, ac efficitur ipsum mauris vitae augue. Aliquam ornare faucibus nibh, a varius mauris mattis id. Nullam eu nibh aliquam, vestibulum neque sed, sagittis mi. Etiam blandit facilisis sagittis. Duis ut dolor sed nibh egestas porta. Aenean quis lorem nec augue semper convallis lacinia eget orci. Nam eget dolor tortor."

unlist(stringr::str_split(lorem_ipsum, pattern = '\\.[[:space:]]'))[1:5]

```

### Additional Considerations Re: Regular Expressions
\begin{itemize}
\item Punctuation need to double-backslashes \par \vspace{2.5mm}
\item They are very literal -- make sure you're considering what you're asking it to retrieve/search. \par \vspace{2.5mm}
\item Some tools/functions for applying regular expressions are different in application (e.g., dplyr, stringr, stringi, tm, etc.) -- make sure you're considering how they approach splits at punctuation! \par \vspace{2.5mm}
\item A lot of this is trial \& error 
\end{itemize}

### Regex Practice
\textbf{Practice Sentence} \par \vspace{2.5mm}
\begin{itemize}
\item \textbf{Sentence 1}: The cautious archivist indexed seven obscure manuscripts before dawn. \par \vspace{2.5mm}
\item \textbf{Sentence 2}: Tomorrow a reckless cyclist shattered records while racing before sunset. \vspace{5mm}
\item Write a single regex to recover the time of day mentioned in each sentence -- and only the time of day (\textit{Hint}: Make sure you remove punctuation...)
\end{itemize}

### Regex Practice (Cont.)
\scriptsize

```{r, regex_practice}
s1 <- 'The cautious archivist indexed seven obscure manuscripts before dawn.'
s2 <- 'Tomorrow a reckless cyclist shattered records while racing before sunset.'

sentences <- c(s1, s2)

gsub('.*before ', '', gsub('\\.', '', sentences))

```





# Retrieving Text

### Lies, Damn Lies, and Statistics (2000)
\centering
\includegraphics[width=0.75\textwidth]{../../../images/west_wing_cast.jpg}

### Lies, Damn Lies, and Statistics (2000)
\begin{itemize}
\item Aaron Sorkin's \textit{The West Wing} ran from 1999-2007 -- easily the best political drama ever created. \par \vspace{2.5mm}
\item \textit{Data for Progress} +23\% Favorable Rating (higher among older, more educated, and higher-income Americans) \par \vspace{2.5mm}
\item 100 Awards from 289 Nominations (27 Emmys, 2 Peabody Awards, 6 SAG Awards, among others) -- it's incredible. 
\end{itemize}


### Lies, Damn Lies, and Statistics (2000)
\begin{itemize}
\item \textit{Lies, Damn Lies, and Statistics} premiered May 10, 2000 \par \vspace{2.5mm}
\item Title sourced from Mark Twain (or Benjamin Disraeli): \textit{There are lies, damned lies, and statistics} -- used to describe instances where people are given credibility for often weak or disagreeable positions by using statistics to sound empirically rigid. \par \vspace{2.5mm}
\item \textbf{Synopsis}: The Bartlet administration anxiously awaits crucial polling data following a shift in strategy, while managing political crises. Despite internal pessimism and a personal scandal involving Sam, the poll shows a surprising nine-point increase in approval. \par \vspace{2.5mm}
\item \href{https://www.youtube.com/watch?v=5fC98J5RQU8&t=51s}{Opening Scene} 
\end{itemize}

### Lies, Damn Lies, and Statistics (2000)
\begin{itemize}
\item Let's imagine I wanted to know which character spoke the most lines. \par \vspace{2.5mm}
\item How could I do that? \pause \par \vspace{2.5mm}
\item Options: 
  \begin{itemize}
  \item Watch the episode and count utterances
  \item Read the closed captioning transcription and (again) count utterances
  \item Recover the close captioning transcript and use our computing resources to accurately (and quickly) analyze 
  \end{itemize}
\end{itemize}


### First Look at Transcript

\begin{columns}[t]
\begin{column}[t]{0.5\textwidth}
\centering
\includegraphics[width=5.5cm]{../../../images/lies_damn_lies_statistics_script.png}
\end{column}
\begin{column}[t]{0.5\textwidth}
\centering
\includegraphics[width=5.75cm]{../../../images/lies_damn_lies_statistics_script_2.png}
\end{column}
\end{columns}


### Goal
\begin{itemize}
\item Clearly, the text is not as organized as I’d like it to be. \par \vspace{2.5mm}
\item \textbf{Remember}: Organizing text data is like deciphering the wording of variables -- you can write generalizable coding routines to parse text but a lot of this work is going to be application-specific \par \vspace{2.5mm}
\item My next steps will be to consciously try to develop a 4-column dataframe that identifies:
  \begin{itemize}
  \item \textbf{Character}: Which character is currently speaking.
  \item \textbf{Dialogue}: The text (string)
  \item \textbf{Word Count}: How many words are found in the text
  \item \textbf{Line Number}: The current number of dialogue entries to that point
  \end{itemize}
\item I'm only interested in Josh, Toby, C.J., Donna, Sam, Leo, and President Bartlet
\end{itemize}

### Plan of Attack
\begin{itemize}
\item Recover .txt file of episode script
\item Convert to dataframe
\item Use a regular expression (regex) to identify \& partition text for each character
\item Count number of utterances and words in each utterance
\end{itemize}

### Recover Script

```{r west_wing_script_url, echo=FALSE}
west_wing_script_location <- "https://raw.githubusercontent.com/JakeTruscott/CSS_POS_UF/main/docs/assets/replication_materials/class_4/supplemental_materials/West_Wing_S1_E21.txt" # Location of txt File on Github Repo
```
\scriptsize
```{r, west_wing_script_grab}

west_wing <- readLines(west_wing_script_location, warn = FALSE) # Read Txt from GitHub Repo

head(west_wing)  # Print Head

```

### Process Script

\scriptsize
```{r, process_west_wing, echo=FALSE}

west_wing <- data.frame(unlist(west_wing)) %>%
  setNames('text') # Unlist Script as Text

characters <- c('Josh', 'Toby', 'C.J.', 'Donna', 'Sam', 'Leo', 'Bartlet')  # Characters of Interests

character_regex <- paste0("^(", paste0(toupper(characters), collapse = "|"), ")$") #

damn_lies <-  west_wing %>%
  mutate(character_line = ifelse(stringr::str_detect(text, character_regex), 1, 0),
         empty_row = ifelse(text == '', 1, 0),
         first_entry = ifelse(character_line == 1, 1, NA)) %>%
  tidyr::fill(first_entry, .direction = 'down') %>%
  filter(!is.na(first_entry)) %>%
  select(-c(first_entry)) %>%
  mutate(group = cumsum(character_line == 1)) %>%
  group_by(group) %>%
  mutate(to_keep = row_number() < which(empty_row == 1)[1] | is.na(which(empty_row == 1)[1])) %>%
  ungroup() %>%
  filter(to_keep) %>%
  select(text, character_line) %>%
  mutate(group = cumsum(character_line == 1)) %>%
  group_by(group) %>%
  summarise(
    character = text[character_line == 1][1],
    dialogue  = paste(text[-1], collapse = " "),
    .groups = "drop") %>%
  rename(id = group) %>%
  select(character, dialogue, id) %>%
  rowwise() %>%
  mutate(word_count = stringr::str_count(dialogue, "\\S+")) %>%
  ungroup() %>%
  filter(!word_count == 0)

```

```{r, process_west_wing_print, echo=FALSE}

head(damn_lies, 10)

```


### Measure Speaker Variance
\scriptsize
```{r, process_west_wing_print_2}

damn_lies %>%
  group_by(character) %>% 
  summarise(total_words = sum(word_count), 
            average_words = round(mean(word_count)), 
            total_lines = n()) %>%
  arrange(desc(total_words)) %>%
  rename(Character = character, 
         `Total Words` = total_words, 
         `Average Words` = average_words, 
         `Total Lines` = total_lines)

# For Each Character -- Summarize Total Words, Avg. Per Utterance, and Total Utterances

```

### Measuring Length of Supreme Court Opinions (Black \& Spriggs 2008)
\begin{itemize}
\item Main reading: Black \& Spriggs 2008 \par \vspace{2.5mm}
\item What is the methodology? \par \vspace{2.5mm}
\item What is the main finding? 
\end{itemize}

### Measuring Words in Supreme Court Oral Arguments (Dobbs v. Jackson)
\textbf{Practice}
\begin{itemize}
\item Using the Supreme Court's argument in \textit{Dobbs v. Jackson} (2021), filter to \texttt{role = justice} and recover the total utterances of each \texttt{speaker}, as well as the total words for each. 
\end{itemize}
\scriptsize
```{r load_dobbs_fake, eval=F}

dobbs <- get(load('data/class_4/dobbs_19-1392.rdata')) # Load Dobbs

```

```{r load_dobbs, include=FALSE}

dobbs <- get(load('../data/dobbs_19-1392.rdata')) # Load Dobbs

```

### Dobbs -- Utterances
\scriptsize
```{r, scotustext_example_utterances}

dobbs %>%
  filter(role == 'Justice') %>%
  group_by(speaker) %>%
  summarise(utterances = n()) %>%
  arrange(desc(utterances)) # Utterances

```


### Dobbs -- Total Words
\scriptsize
```{r, scotustext_example_words}

dobbs %>%
  filter(role == 'Justice') %>%
  group_by(speaker) %>%
  summarize(text = paste(text, collapse = " "), .groups = "drop") %>%
  tidytext::unnest_tokens(word, text) %>%
  group_by(speaker) %>%
  summarise(word_count = n(), .groups = 'drop') %>%
  arrange(desc(word_count)) # Words Spoken

```


### gutenbergr Repository
\begin{itemize}
\item Interfaces with Project Gutenberg to download public-domain texts directly into \texttt{R} \par \vspace{2.5mm}
\item Returns texts in tidy data frames, making them easy to merge, filter, and analyze \par \vspace{2.5mm}
\item Supports metadata queries (author, title, language, subject, ID) \par \vspace{2.5mm}
\item Ideal for large-scale text analysis and reproducible workflows \par \vspace{2.5mm}
\end{itemize}

### gutenbergr Repository (Cont.)
\scriptsize
```{r, gutenberger_oliver_twist}
library(gutenbergr)
gutenberg_metadata %>%
  filter(title == "Oliver Twist")

```
\scriptsize
```{r gutenberger_oliver_twist_no_print, eval = F}

oliver_twist <- gutenberg_download(730) # Download Oliver Twist

```

### Practice -- \textit{Oliver Twist} by Charles Dickens
\textbf{Practice Task -- Work w/ Classmate} \par \vspace{2.5mm}
\begin{itemize}
\item Using \texttt{gutenbergr} -- Recover the text from \textit{Olive Twist} by Charles Dickens
\item Construct a regular expression to identify chapters and breaks (\textit{Hint}: Use \texttt{regex} cheat sheet!)
\item Partition the text to two columns -- \texttt{Chapter} \& \texttt{Text}
\item Return table using \texttt{stargazer} to identify the total and unique volume of words for each chapter.
\end{itemize}

### Practice -- \textit{War and Peace} by Leo Tolstoy

\textbf{Practice Task -- Work w/ Classmate} \par \vspace{2.5mm}
\begin{itemize}
\item Using \texttt{gutenbergr} -- Recover the text from \textit{War \& Peace} by Leo Tolstoy
\item Construct a regular expression to identify chapters and breaks (\textit{Hint}: Use \texttt{regex} cheat sheet!)
\item Partition the text to two columns -- \texttt{Chapter} \& \texttt{Text}
\item Return table using \texttt{stargazer} to identify the total and unique volume of words for each chapter.
\end{itemize}

### Practice -- Pick Another Book! 
\centering
\large
\textbf{Complete the same task (again) but with a book or document of your choice (\texttt{gutenbergr} -- You and a classmate will present it to the class. }


# Corpus Creation

### The Pipeline
\large
\centering
Raw Text → Corpus (\textbf{You're HERE}) → Tokens → Features (DFM) → Models / Analysis


### Corpus 
\begin{itemize}
\item \textbf{Corpus} A structured \& systematic collection of texts that you treat as data rather than as individual documents. \par \vspace{2.5mm}
\item \textbf{Main Idea}: Once text is in a corpus, you stop reading it line-by-line and start analyzing patterns across many texts. \par \vspace{2.5mm}
\item Allows for multiple texts to assume consistent (comparable) structure and includes associated metadata 
\item We're going to practice today constructing a corpus with \texttt{quanteda}
\end{itemize}

### Sample Corpus Using Quanteda
\scriptsize
```{r, quanteda_sample}

library(quanteda) # Load Quanteda

texts <- c(
  "The quick brown fox jumps over the lazy dog.",
  "Data science is revolutionizing the way we analyze information.",
  "Text analysis in R is fun and informative!"
) # Sample Texts (as vector)

texts_with_meta <- tibble(
  doc_id = c("sentence_1", "sentence_2", "sentence_3"),
  text = texts,
  author = c('Josh', 'Leo', 'Toby'),
  date = as.Date(c("2025-01-01", "2025-01-02", "2025-01-03"))
) # Create Metadata for Texts (Same as tm example!)

quanteda_corpus <- corpus(texts_with_meta, text_field = "text")

```


### Sample Corpus Using Quanteda
\scriptsize
```{r quanteda_sample_cont, echo=FALSE}

summary(quanteda_corpus) # Inspect the Corpus

```

### Lies, Damn Lies, and Statistics (Again!)
\scriptsize
```{r damn_lies_corpus}


damn_lies_corpus <- quanteda::corpus(damn_lies, text_field = "dialogue") # Create Corpus (Text = 'dialogue')

summary(damn_lies_corpus[1:10]) # Inspect (Just First Couple of Rows)

```


### Lies, Damn Lies, and Statistics (Again! Cont.)

```{r damn_lies_corpus_cont}


damn_lies_corpus[1]
quanteda::docvars(damn_lies_corpus[1])

```


### Corpus Practice
\centering
\large
\textbf{Using one of the \texttt{gutenbergr} texts from today -- Construct a corpus, including both the text and the chapter metadata}





# Looking Forward

### Looking Forward (Next Class)
\begin{itemize}
\item The Bag of Words -- Or, how we can use words for more than just descriptive statistics. 
\end{itemize}


